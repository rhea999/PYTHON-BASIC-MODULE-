{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 1:** What is Anomaly Detection? Explain its types (point,\n",
    "> contextual, and collective anomalies) with examples.\n",
    ">\n",
    "> **Answer 1 :-**  \n",
    "> **Anomaly detection** is the process of identifying unusual patterns,\n",
    "> events, or observations in data that do not follow the expected\n",
    "> behavior.\n",
    ">\n",
    "> These unusual observations are called **anomalies** (or outliers).\n",
    ">\n",
    "> In real life, anomalies often indicate something important, like:\n",
    ">\n",
    "> ●​ Fraudulent transactions in banking​\n",
    ">\n",
    "> ●​ Faulty sensors in manufacturing​\n",
    ">\n",
    "> ●​ Sudden spikes in website traffic​\n",
    ">\n",
    "> ●​ Rare diseases in healthcare\n",
    ">\n",
    "> **Types of Anomalies**\n",
    ">\n",
    "> **1. Point Anomalies**\n",
    ">\n",
    "> ●​ A **single data point** is very different from the rest of the data.​\n",
    ">\n",
    "> ●​ Most common type of anomaly.​\n",
    ">\n",
    "> **Example:**\n",
    ">\n",
    "> ●​ In a dataset of monthly credit card transactions where most users\n",
    "> spend between ₹5,000–₹20,000, a transaction of **₹2,00,000** would be\n",
    "> a point anomaly.​\n",
    ">\n",
    "> ●​ A sensor in a machine showing a sudden temperature of **200°C** when\n",
    "> normal readings are around 30–40°C.\n",
    ">\n",
    "> **2. Contextual Anomalies**\n",
    ">\n",
    "> ●​ A data point is unusual **only in a specific context** (depends on\n",
    "> time, location, or situation).​\n",
    ">\n",
    "> ●​ Common in **time-series data**.​\n",
    ">\n",
    "> **Example:**\n",
    ">\n",
    "> ●​ A temperature of **30°C** is normal in summer but abnormal in winter\n",
    "> in Delhi.​\n",
    ">\n",
    "> ●​ An employee logging in at **3:00 AM** might be unusual (context:\n",
    "> working hours), but normal for a night-shift worker.\n",
    ">\n",
    "> **3. Collective Anomalies**\n",
    ">\n",
    "> ●​ A **group of data points together** form an anomaly, even though\n",
    "> individual points may look normal.​\n",
    ">\n",
    "> ●​ Often indicate **sequential or group-based unusual behavior**.​\n",
    ">\n",
    "> **Example:**\n",
    ">\n",
    "> ●​ In network traffic monitoring, a **sudden burst of packets over 10\n",
    "> minutes** may indicate a **DDoS attack**.​\n",
    ">\n",
    "> ●​ In stock market data, a series of small but consistent drops in\n",
    "> stock price could indicate insider trading.​\n",
    ">\n",
    "> **Question 2:** Compare Isolation Forest, DBSCAN, and Local Outlier\n",
    "> Factor in terms of their approach and suitable use cases.\n",
    ">\n",
    "> **Answer 2 :-**\n",
    ">\n",
    "> **Comparison of Isolation Forest, DBSCAN,**\n",
    ">\n",
    "> **and Local Outlier Factor**\n",
    ">\n",
    "> **1. Isolation Forest (IF)**\n",
    ">\n",
    "> ●​ **Approach:​**\n",
    ">\n",
    "> ○​ Based on the principle that anomalies are **easier to isolate** than\n",
    "> normal points.​\n",
    ">\n",
    "> ○​ Builds random decision trees; anomalies end up in shorter paths\n",
    "> because they are rare and distinct.​  \n",
    "> ●​ **Type:** Ensemble method (tree-based).​  \n",
    "> ●​ **Use Cases:​**  \n",
    "> ○​ Large, high-dimensional datasets.​  \n",
    "> ○​ Fraud detection (credit card, banking).​  \n",
    "> ○​ Intrusion detection in cybersecurity.​  \n",
    "> ●​ **Strengths:​**  \n",
    "> ○​ Scales well to big data.​  \n",
    "> ○​ Works without distance calculation (better in high dimensions).​  \n",
    "> ●​ **Limitations:​**  \n",
    "> ○​ Assumes anomalies are few and different in distribution.​  \n",
    "> ○​ Not good for finding **clusters of anomalies**.​\n",
    ">\n",
    "> **2. DBSCAN (Density-Based Spatial Clustering of Applications with\n",
    "> Noise)** ●​ **Approach:​**  \n",
    "> ○​ Groups together points that are close (high density).​  \n",
    "> ○​ Points in low-density regions are considered anomalies (noise).​ ●​\n",
    "> **Type:** Density-based clustering.​  \n",
    "> ●​ **Use Cases:​**\n",
    ">\n",
    "> ○​ Spatial/geographical data (e.g., detecting outlier GPS locations).​  \n",
    "> ○​ Clustering + anomaly detection simultaneously.​  \n",
    "> ○​ Irregularly shaped clusters.​  \n",
    "> ●​ **Strengths:​**  \n",
    "> ○​ Finds both clusters and anomalies.​  \n",
    "> ○​ No need to specify number of clusters (unlike K-Means).​  \n",
    "> ●​ **Limitations:​**  \n",
    "> ○​ Struggles with high-dimensional data.​  \n",
    "> ○​ Sensitive to parameters **ε (neighborhood radius)** and **MinPts\n",
    "> (minimum** **points in a cluster)**.​\n",
    ">\n",
    "> **3. Local Outlier Factor (LOF)**  \n",
    "> ●​ **Approach:​**  \n",
    "> ○​ Compares the **local density** of a point with that of its\n",
    "> neighbors.​ ○​ If a point has much lower density than neighbors → it’s\n",
    "> an anomaly.​ ●​ **Type:** Density-based local anomaly detection.​  \n",
    "> ●​ **Use Cases:​**  \n",
    "> ○​ Detecting local anomalies where data is not globally uniform.​ ○​\n",
    "> Fraud detection when some regions are denser than others.​ ○​\n",
    "> Medical/biological data where clusters have varying densities.​\n",
    ">\n",
    "> ●​ **Strengths:​**\n",
    ">\n",
    "> ○​ Works well for datasets with varying density.​\n",
    ">\n",
    "> ○​ Detects **contextual anomalies**.​\n",
    ">\n",
    "> ●​ **Limitations:​**\n",
    ">\n",
    "> ○​ Computationally expensive for large datasets.​\n",
    ">\n",
    "> ○​ Sensitive to parameter **k (number of neighbors)**.​\n",
    ">\n",
    "> **Question 3:** What are the key components of a Time Series? Explain\n",
    "> each with one example.\n",
    ">\n",
    "> **Answer 3 :-**\n",
    ">\n",
    "> **Key Components of a Time Series**\n",
    ">\n",
    "> A **time series** is a sequence of data points collected or recorded\n",
    "> at specific time intervals (daily, monthly, yearly, etc.).​  \n",
    "> It usually has **four main components**:\n",
    ">\n",
    "> **1. Trend (T)**\n",
    ">\n",
    "> ●​ The **long-term direction** of the data (upward, downward, or\n",
    "> stable) over a long period.​\n",
    ">\n",
    "> ●​ Shows the overall growth or decline.​\n",
    ">\n",
    "> **Example:**\n",
    ">\n",
    "> ●​ The number of internet users in India has been **increasing\n",
    "> steadily** over the last 20 years.​\n",
    ">\n",
    "> ●​ Stock market index (like NIFTY 50) shows a long-term upward trend\n",
    "> despite short-term fluctuations.​\n",
    ">\n",
    "> **2. Seasonality (S)**  \n",
    "> ●​ **Regular, repeating patterns** in data that occur at fixed\n",
    "> intervals (daily, weekly, monthly, yearly).​  \n",
    "> ●​ Caused by seasonal factors such as weather, festivals, holidays,\n",
    "> etc.​\n",
    ">\n",
    "> **Example:**  \n",
    "> ●​ Ice cream sales **increase every summer** and drop in winter.​●​\n",
    "> E-commerce sales **spike during Diwali and Christmas seasons**.​\n",
    ">\n",
    "> **3. Cyclic Component (C)**  \n",
    "> ●​ **Long-term oscillations** in data that are not strictly periodic\n",
    "> (unlike seasonality).​●​ Often linked to **business cycles or economic\n",
    "> trends**.​  \n",
    "> ●​ Duration is more than a year and not fixed.​\n",
    ">\n",
    "> **Example:**  \n",
    "> ●​ Economic cycles of **boom → recession → recovery** affect\n",
    "> unemployment rates.​●​ Real estate prices going up and down over decades\n",
    "> due to market cycles.​\n",
    ">\n",
    "> **4. Irregular/Random Component (I)**  \n",
    "> ●​ Unpredictable, random variations in data due to **unexpected\n",
    "> events**.​●​ Cannot be explained by trend, seasonality, or cycles.​\n",
    ">\n",
    "> **Example:**\n",
    ">\n",
    "> ●​ Sudden drop in airline travel due to **COVID-19 pandemic**.​\n",
    ">\n",
    "> ●​ A natural disaster causing unusual spikes in demand for certain\n",
    "> goods.\n",
    ">\n",
    "> **Question 4:** Define Stationary in time series. How can you test and\n",
    "> transform a non-stationary series into a stationary one?\n",
    ">\n",
    "> **Answer 4 :-**\n",
    ">\n",
    "> A **stationary time series** is one whose **statistical properties**\n",
    "> (like mean, variance, and autocorrelation) do not change over time.​  \n",
    "> In simple words → the series looks \"similar\" throughout, without\n",
    "> long-term trends or changing variance.\n",
    ">\n",
    "> Stationarity is important because many time series forecasting models\n",
    "> (like ARIMA) assume the data is stationary.\n",
    ">\n",
    "> **Types of Stationarity**\n",
    ">\n",
    "> 1.​ **Strict Stationarity** – The complete probability distribution\n",
    "> remains constant over time. (Rare in practice)​\n",
    ">\n",
    "> 2.​ **Weak Stationarity** – Only the first two moments (mean and\n",
    "> variance) are constant, and autocovariance depends only on the lag,\n",
    "> not time. (Most commonly tested)\n",
    ">\n",
    "> **How to Test Stationarity**\n",
    ">\n",
    "> **1. Visual Inspection**\n",
    ">\n",
    "> ●​ Plot the time series.​\n",
    ">\n",
    "> ●​ If it shows a clear **trend, seasonality, or changing variance**,\n",
    "> it’s likely **non-stationary**.​\n",
    ">\n",
    "> 📌 Example: Sales data increasing over years → non-stationary.\n",
    ">\n",
    "> **2. Summary Statistics**\n",
    ">\n",
    "> ●​ Split data into two halves and compare **mean and variance**.​\n",
    ">\n",
    "> ●​ If they differ significantly → non-stationary.​\n",
    ">\n",
    "> **3. Statistical Tests**  \n",
    "> ●​ **Augmented Dickey-Fuller (ADF) Test​**  \n",
    "> ○​ Null Hypothesis (H0): Series has a unit root (non-stationary).​ ○​\n",
    "> Alternative (H1): Series is stationary.​  \n",
    "> ●​ **KPSS Test (Kwiatkowski-Phillips-Schmidt-Shin)​**  \n",
    "> ○​ Null Hypothesis (H0): Series is stationary.​  \n",
    "> ○​ Alternative (H1): Series is non-stationary.​\n",
    ">\n",
    "> 👉 Often, both tests are used together for confirmation.\n",
    ">\n",
    "> 🔹 **How to Transform a Non-Stationary Series into Stationary**  \n",
    "> If the series is non-stationary, we can apply transformations:  \n",
    "> **1. Differencing**  \n",
    "> ●​ Subtract current value from previous value:​\n",
    "\n",
    "<table style=\"width:100%;\">\n",
    "<colgroup>\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th>​</th>\n",
    "<th>​</th>\n",
    "<th>​</th>\n",
    "<th>​</th>\n",
    "<th>​</th>\n",
    "<th><blockquote>\n",
    "<p>Yt′​=Yt​−Yt−1​</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> ●​ Removes trend/seasonality.​  \n",
    "> Example: Stock prices → take difference to stabilize.​\n",
    ">\n",
    "> **2. Log Transformation**\n",
    ">\n",
    "> ●​ Apply log to reduce **variance fluctuations**.​  \n",
    "> Example: Sales data with exponential growth → apply log to compress\n",
    "> large values.​\n",
    ">\n",
    "> **3. Seasonal Differencing**\n",
    ">\n",
    "> ●​ Subtract value from its seasonal lag:​\n",
    "\n",
    "<table style=\"width:100%;\">\n",
    "<colgroup>\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "<col style=\"width: 16%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th>​</th>\n",
    "<th>​</th>\n",
    "<th>​</th>\n",
    "<th>​</th>\n",
    "<th>​</th>\n",
    "<th><blockquote>\n",
    "<p>Yt′​=Yt​−Yt−m​</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> (where m = seasonal period, e.g., 12 for monthly data).\n",
    ">\n",
    "> Example: Monthly electricity consumption (subtract this month’s value\n",
    "> from last year’s same month).\n",
    ">\n",
    "> **4. Detrending**\n",
    ">\n",
    "> ●​ Fit a regression line to capture trends and remove them.​ Example:\n",
    "> Remove linear upward trend in GDP data.​\n",
    ">\n",
    "> **5. Box-Cox Transformation**\n",
    ">\n",
    "> ●​ General power transformation to stabilize variance.​\n",
    ">\n",
    "> **Question 5:** Differentiate between AR, MA, ARIMA, SARIMA, and\n",
    "> SARIMAX models in terms of structure and application.\n",
    ">\n",
    "> **Answer 5 :-**\n",
    ">\n",
    "> **1. AR (Autoregressive)**\n",
    ">\n",
    "> ●​ Uses past values to predict the future.​\n",
    ">\n",
    "> ●​ Example: Tomorrow’s temperature depends on yesterday’s and today’s\n",
    "> temperature.​\n",
    ">\n",
    "> ●​ Good when data depends on its own history.\n",
    ">\n",
    "> **2. MA (Moving Average)**\n",
    ">\n",
    "> ●​ Uses **past errors (shocks)** to predict the future.​\n",
    ">\n",
    "> ●​ Example: Today’s sales depend on unexpected events (errors) in the\n",
    "> last few days.​\n",
    ">\n",
    "> ●​ Good for **short-term noise smoothing**.\n",
    ">\n",
    "> **3. ARIMA (AutoRegressive Integrated Moving Average)**\n",
    ">\n",
    "> ●​ Combines **AR + MA + Differencing**.​\n",
    ">\n",
    "> ●​ Handles **trend** in data (non-stationary series).​\n",
    ">\n",
    "> ●​ Example: Forecasting GDP growth, stock prices, sales.\n",
    ">\n",
    "> **5. SARIMAX (Seasonal ARIMA with Exogenous Variables)**\n",
    ">\n",
    "> ●​ SARIMA + **external factors (X)**.​\n",
    ">\n",
    "> ●​ Uses outside information that affects the series.​\n",
    ">\n",
    "> ●​ Example: Electricity demand (depends on temperature), sales (depends\n",
    "> on holidays or ads).​\n",
    ">\n",
    "> **Dataset:**  \n",
    "> **● NYC Taxi Fare Data**  \n",
    "> **● AirPassengers Dataset**  \n",
    "> **Question 6:** Load a time series dataset (e.g., AirPassengers), plot\n",
    "> the original series, and decompose it into trend, seasonality, and\n",
    "> residual components\n",
    ">\n",
    "> **Answer 6 :-**  \n",
    "> \\# Step 1: Import libraries  \n",
    "> import pandas as pd  \n",
    "> import matplotlib.pyplot as plt  \n",
    "> from statsmodels.tsa.seasonal import seasonal_decomposeimport\n",
    "> statsmodels.api as sm\n",
    ">\n",
    "> \\# Step 2: Load the AirPassengers dataset  \n",
    "> \\# (available in statsmodels)  \n",
    "> data = sm.datasets.airpassengers.load_pandas().data\n",
    ">\n",
    "> \\# Convert 'Month' to datetime  \n",
    "> data\\['Month'\\] = pd.to_datetime(data\\['Month'\\])  \n",
    "> data.set_index('Month', inplace=True)\n",
    ">\n",
    "> \\# Step 3: Plot original series  \n",
    "> plt.figure(figsize=(10,5))  \n",
    "> plt.plot(data\\['AirPassengers'\\], label=\"AirPassengers\")  \n",
    "> plt.title(\"AirPassengers Dataset (1949-1960)\")  \n",
    "> plt.xlabel(\"Year\")  \n",
    "> plt.ylabel(\"Number of Passengers\")  \n",
    "> plt.legend()  \n",
    "> plt.show()\n",
    ">\n",
    "> \\# Step 4: Decompose into trend, seasonality, residuals  \n",
    "> decomposition = seasonal_decompose(data\\['AirPassengers'\\],\n",
    "> model='multiplicative',period=12)\n",
    ">\n",
    "> \\# Step 5: Plot decomposition  \n",
    "> decomposition.plot()  \n",
    "> plt.show()\n",
    ">\n",
    "> **What You’ll See in the Output:**\n",
    ">\n",
    "> 1.​ **Original Series** → Increasing passengers with strong\n",
    "> seasonality.​\n",
    ">\n",
    "> 2.​ **Trend** → Long-term upward growth in air travel.​\n",
    ">\n",
    "> 3.​ **Seasonality** → Repeating yearly pattern (summer/winter peaks).​\n",
    ">\n",
    "> 4.​ **Residual** → Random fluctuations not explained by\n",
    "> trend/seasonality.\n",
    ">\n",
    "> **Question 7:** Apply Isolation Forest on a numerical dataset (e.g.,\n",
    "> NYC Taxi Fare) to detect anomalies. Visualize the anomalies on a 2D\n",
    "> scatter plot.\n",
    ">\n",
    "> **Answer 7 :-**  \n",
    "> \\# Step 1: Import libraries  \n",
    "> import pandas as pd  \n",
    "> import matplotlib.pyplot as plt  \n",
    "> from sklearn.ensemble import IsolationForest\n",
    ">\n",
    "> \\# Step 2: Load dataset (NYC Taxi Fare)  \n",
    "> \\# Example: If CSV file is available  \n",
    "> \\# data = pd.read_csv(\"nyc_taxi_fare.csv\")\n",
    ">\n",
    "> \\# For demonstration, let's simulate small taxi fare-like datasetdata\n",
    "> = pd.DataFrame({\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"fare_amount\": [5, 8, 7, 6, 15, 7, 6, 300, 10, 9, 7, 5, 400, 8, 6], #\n",
    "Some extreme fares</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> = anomalies\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"trip_distance\": [1, 2, 1.5, 2, 3, 1.8, 2, 0.5, 2.5, 3, 2.2, 1.7,\n",
    "0.3, 2, 1.6]</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> })\n",
    ">\n",
    "> \\# Step 3: Apply Isolation Forest  \n",
    "> iso_forest = IsolationForest(contamination=0.1, random_state=42) \\#\n",
    "> contamination \\~expected % of anomalies  \n",
    "> data\\['anomaly'\\] = iso_forest.fit_predict(data\\[\\['fare_amount',\n",
    "> 'trip_distance'\\]\\])\n",
    ">\n",
    "> \\# anomaly = -1 (outlier), 1 (normal)  \n",
    "> outliers = data\\[data\\['anomaly'\\] == -1\\]  \n",
    "> inliers = data\\[data\\['anomaly'\\] == 1\\]\n",
    ">\n",
    "> \\# Step 4: Visualize anomalies  \n",
    "> plt.figure(figsize=(8,6))  \n",
    "> plt.scatter(inliers\\['trip_distance'\\], inliers\\['fare_amount'\\],\n",
    "> c='blue', label='Normal',alpha=0.6)  \n",
    "> plt.scatter(outliers\\['trip_distance'\\], outliers\\['fare_amount'\\],\n",
    "> c='red', label='Anomaly',marker='x', s=100)  \n",
    "> plt.title(\"Isolation Forest - NYC Taxi Fare Anomaly Detection\")  \n",
    "> plt.xlabel(\"Trip Distance (miles)\")  \n",
    "> plt.ylabel(\"Fare Amount (\\$)\")  \n",
    "> plt.legend()  \n",
    "> plt.show()\n",
    ">\n",
    "> **What Happens Here:**\n",
    ">\n",
    "> ●​ **Input Features** → fare_amount and trip_distance​\n",
    ">\n",
    "> ●​ **Isolation Forest** marks unusual points as anomalies (label = -1)​\n",
    ">\n",
    "> ●​ **Scatter Plot**:​\n",
    ">\n",
    "> ○​ Blue = normal fares​\n",
    ">\n",
    "> ○​ Red (X) = anomalies (e.g., super high fares like \\$300 or \\$400 for\n",
    "> a short trip)\n",
    ">\n",
    "> **Expected Output**\n",
    ">\n",
    "> **Scatter Plot**  \n",
    "> ●​ **Blue points** → Normal taxi fares (around \\$5–\\$15 for short\n",
    "> trips).​\n",
    ">\n",
    "> ●​ **Red X points** → Anomalies detected (e.g., fares of **\\$300** and\n",
    "> **\\$400** for short trips).​\n",
    ">\n",
    "> It would look like this (illustration):  \n",
    "> Fare Amount (\\$)  \n",
    "> ↑  \n",
    "> \\| X (400\\$ anomaly)  \n",
    "> \\| X (300\\$ anomaly)  \n",
    "> \\|  \n",
    "> \\| ● ● ● ● ●  \n",
    "> \\| ● ● ● ● ●  \n",
    "> +--------------------------------→ Trip Distance (miles)\n",
    ">\n",
    "> ●​ Most fares cluster around **(trip distance 1–3, fare \\$5–15)**.​\n",
    ">\n",
    "> ●​ Anomalies (300, 400) are marked **red X** far away from the normal\n",
    "> cluster.​\n",
    ">\n",
    "> **Question 8:** Train a SARIMA model on the monthly airline passengers\n",
    "> dataset.\n",
    ">\n",
    "> Forecast the next 12 months and visualize the results.\n",
    ">\n",
    "> **Answer 8 :-**  \n",
    "> import pandas as pd  \n",
    "> import matplotlib.pyplot as plt  \n",
    "> import statsmodels.api as sm  \n",
    "> from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    ">\n",
    "> \\# Load built-in AirPassengers dataset  \n",
    "> data = sm.datasets.airpassengers.load_pandas().data\n",
    ">\n",
    "> \\# Convert 'Month' to datetime and set as index  \n",
    "> data\\['Month'\\] = pd.to_datetime(data\\['Month'\\])  \n",
    "> data.set_index('Month', inplace=True)\n",
    ">\n",
    "> \\# Rename column for convenience  \n",
    "> data = data.rename(columns={\"AirPassengers\": \"Passengers\"})\n",
    ">\n",
    "> \\# Fit SARIMA model  \n",
    "> model = SARIMAX(data\\['Passengers'\\], order=(1,1,1),\n",
    "> seasonal_order=(1,1,1,12))results = model.fit(disp=False)  \n",
    "> print(results.summary())\n",
    ">\n",
    "> \\# Forecast for next 12 months\n",
    ">\n",
    "> forecast = results.get_forecast(steps=12)  \n",
    "> forecast_index =\n",
    "> pd.date_range(start=data.index\\[-1\\]+pd.DateOffset(months=1),periods=12,\n",
    "> freq='M')\n",
    ">\n",
    "> \\# Get predicted mean and confidence intervals  \n",
    "> forecast_mean = forecast.predicted_mean  \n",
    "> forecast_ci = forecast.conf_int()\n",
    ">\n",
    "> plt.figure(figsize=(10,6))  \n",
    "> plt.plot(data.index, data\\['Passengers'\\], label=\"Original Data\")  \n",
    "> plt.plot(forecast_index, forecast_mean, label=\"Forecast\",\n",
    "> color='red')plt.fill_between(forecast_index,\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>forecast_ci.iloc[:, 0],</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>forecast_ci.iloc[:, 1],</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>color='pink', alpha=0.3, label=\"Confidence Interval\")</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> plt.title(\"SARIMA Forecast - AirPassengers Dataset\")  \n",
    "> plt.xlabel(\"Year\")  \n",
    "> plt.ylabel(\"Number of Passengers\")  \n",
    "> plt.legend()  \n",
    "> plt.show()\n",
    ">\n",
    "> **Expected Output**\n",
    ">\n",
    "> The **plot** will show:\n",
    ">\n",
    "> ●​ **Blue line** → Original passenger data (1949–1960).​\n",
    ">\n",
    "> ●​ **Red line** → Forecasted passenger numbers for the next 12 months\n",
    "> (1961).​\n",
    ">\n",
    "> ●​ **Shaded pink region** → Confidence interval (uncertainty in\n",
    "> forecast).\n",
    ">\n",
    "> The forecast continues the **upward trend + seasonality** (higher\n",
    "> values in summer, lower in winter).\n",
    ">\n",
    "> Typical forecasted values (approximate, depending on SARIMA tuning):\n",
    ">\n",
    "> **Question 9**: Apply Local Outlier Factor (LOF) on any numerical\n",
    "> dataset to detect anomalies and visualize them using matplotlib.\n",
    ">\n",
    "> **Answer 9 :-**  \n",
    "> import numpy as np  \n",
    "> import pandas as pd  \n",
    "> import matplotlib.pyplot as plt  \n",
    "> from sklearn.neighbors import LocalOutlierFactor\n",
    ">\n",
    "> \\# Simulated dataset (fare vs. distance)  \n",
    "> data = pd.DataFrame({\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"fare_amount\": [5, 6, 7, 8, 10, 15, 6, 7, 8, 9, 12, 300, 400, 5, 7],\n",
    "# anomalies = 300,</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> 400\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"trip_distance\": [1, 1.5, 2, 2.2, 2.5, 3, 1.7, 1.8, 2.3, 2.1, 3.2,\n",
    "0.5, 0.3, 1.2, 1.6]</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> })\n",
    ">\n",
    "> \\# Initialize LOF  \n",
    "> lof = LocalOutlierFactor(n_neighbors=5, contamination=0.1)\n",
    ">\n",
    "> \\# Fit and predict  \n",
    "> data\\['anomaly'\\] = lof.fit_predict(data\\[\\['fare_amount',\n",
    "> 'trip_distance'\\]\\])\n",
    ">\n",
    "> \\# Separate normal and anomalies  \n",
    "> outliers = data\\[data\\['anomaly'\\] == -1\\]  \n",
    "> inliers = data\\[data\\['anomaly'\\] == 1\\]\n",
    ">\n",
    "> plt.figure(figsize=(8,6))  \n",
    "> plt.scatter(inliers\\['trip_distance'\\], inliers\\['fare_amount'\\],\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>c='blue', label='Normal', alpha=0.6)</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> plt.scatter(outliers\\['trip_distance'\\], outliers\\['fare_amount'\\],\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>c='red', label='Anomaly', marker='x', s=100)</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> plt.title(\"Local Outlier Factor (LOF) - Anomaly Detection\")  \n",
    "> plt.xlabel(\"Trip Distance (miles)\")  \n",
    "> plt.ylabel(\"Fare Amount (\\$)\")  \n",
    "> plt.legend()  \n",
    "> plt.show()\n",
    ">\n",
    "> **Expected Output**\n",
    ">\n",
    "> Scatter Plot will show:\n",
    ">\n",
    "> ●​ **Blue points** → Normal data (fare \\~ \\$5–15 for trips 1–3 miles).​\n",
    ">\n",
    "> ●​ **Red X points** → Anomalies (fares \\$300 & \\$400 at very short\n",
    "> distances).​\n",
    ">\n",
    "> It would look something like this (conceptual sketch):\n",
    ">\n",
    "> Fare Amount (\\$)  \n",
    "> ↑  \n",
    "> \\| X (400 anomaly)  \n",
    "> \\| X (300 anomaly)  \n",
    "> \\|  \n",
    "> \\| ● ● ● ● ●  \n",
    "> \\| ● ● ● ●  \n",
    "> +--------------------------------→ Trip Distance (miles)\n",
    "\n",
    "**Question 10:** You are working as a data scientist for a power grid\n",
    "monitoring company.\n",
    "\n",
    "> Your goal is to forecast energy demand and also detect abnormal spikes\n",
    "> or drops in real-time consumption data collected every 15 minutes. The\n",
    "> dataset includes features like timestamp, region, weather conditions,\n",
    "> and energy usage. Explain your real-time data science workflow:  \n",
    "> ● How would you detect anomalies in this streaming data (Isolation\n",
    "> Forest / LOF / DBSCAN)?\n",
    ">\n",
    "> ● Which time series model would you use for short-term forecasting\n",
    "> (ARIMA / SARIMA / SARIMAX)?\n",
    ">\n",
    "> ● How would you validate and monitor the performance over time?\n",
    ">\n",
    "> ● How would this solution help business decisions or operations?\n",
    ">\n",
    "> **ANSWER 10 :-**\n",
    ">\n",
    "> **Problem:**\n",
    ">\n",
    "> We have **streaming energy consumption data (15-min intervals)** with\n",
    "> features like **timestamp, region, weather, usage**.​  \n",
    "> We need to:\n",
    ">\n",
    "> 1.​ Forecast short-term demand.​\n",
    ">\n",
    "> 2.​ Detect abnormal spikes/drops in real time.​\n",
    ">\n",
    "> 3.​ Ensure model is validated and monitored continuously.\n",
    ">\n",
    "> **1. Anomaly Detection in Streaming Data**\n",
    ">\n",
    "> ●​ **Choice of Algorithm:​**\n",
    ">\n",
    "> ○​ **Isolation Forest** → scalable, works well in real time, handles\n",
    "> high-dimensional data.​\n",
    ">\n",
    "> ○​ **LOF (Local Outlier Factor)** → detects local/contextual anomalies\n",
    "> (e.g., one region consuming abnormally compared to neighbors).​\n",
    ">\n",
    "> ○​ **DBSCAN** → good for clustering + anomaly detection, but less\n",
    "> suitable for streaming (requires re-fitting).​\n",
    ">\n",
    "> **Best fit here:Isolation Forest (real-time & scalable)** + optionally\n",
    "> **LOF** for regional contextual anomalies.\n",
    ">\n",
    "> Example: If demand usually ranges 100–200 MW, and suddenly spikes to\n",
    "> 500 MW in one region while weather is normal → anomaly.\n",
    ">\n",
    "> **2. Short-Term Forecasting Model**\n",
    ">\n",
    "> ●​ Since data is **time-series with seasonality (daily, weekly) +\n",
    "> external factors** **(weather)**:​\n",
    ">\n",
    "> ○​ **ARIMA** → handles trend, no seasonality.​  \n",
    "> ○​ **SARIMA** → handles trend + seasonality.​  \n",
    "> ○​ **SARIMAX** → handles trend + seasonality + external regressors\n",
    "> (like weather).​\n",
    ">\n",
    "> **Best fit here:SARIMAX** (because weather strongly impacts energy\n",
    "> usage). Example: Forecast next 1–3 hours (4–12 intervals) of demand\n",
    "> for scheduling power generation.\n",
    ">\n",
    "> **3. Validation & Monitoring**  \n",
    "> ●​ **Validation during training:​**  \n",
    "> ○​ Use **rolling forecast origin (walk-forward validation)** instead of\n",
    "> simple train/test split.​  \n",
    "> ○​ Metrics: **MAE, RMSE, MAPE** for forecast accuracy.​  \n",
    "> ●​ **Monitoring in production:​**  \n",
    "> ○​ Track **forecast error over time** (drift detection).​  \n",
    "> ○​ Re-train model periodically (weekly/monthly).​  \n",
    "> ○​ Monitor anomaly detection **false positives/negatives**.​\n",
    "\n",
    "Example: If the model consistently underestimates peak hours, retrain\n",
    "with latest data.\n",
    "\n",
    "> **4. Business Value & Operations Impact**  \n",
    "> ●​ **For Grid Stability:​**  \n",
    "> ○​ Detect abnormal spikes/drops → avoid **blackouts** or **equipment**\n",
    "> **overload**.​\n",
    ">\n",
    "> ●​ **For Power Generation Planning:​**  \n",
    "> ○​ Forecast demand → schedule **power plants, renewable energy**\n",
    "> **integration, battery storage**.​  \n",
    "> ●​ **For Cost Optimization:​**  \n",
    "> ○​ Avoid overproduction or underproduction → saves fuel, reduces\n",
    "> wastage.​●​ **For Customers & Policy Makers:​**  \n",
    "> ○​ Better demand-response programs (adjust pricing during peak hours).​\n",
    ">\n",
    "> Example: If anomaly detection finds sudden drop in one region → could\n",
    "> mean equipment failure → send alert to engineers.\n",
    ">\n",
    "> **Final Workflow Summary**  \n",
    "> 1.​ **Ingest streaming data (Kafka, Spark Streaming, etc.).​**  \n",
    "> 2.​ **Anomaly detection:** Isolation Forest (global anomalies), LOF\n",
    "> (contextual anomalies).​  \n",
    "> 3.​ **Forecasting:** SARIMAX (uses seasonality + weather + region\n",
    "> features).​ 4.​ **Validation:** Walk-forward validation, RMSE/MAPE\n",
    "> tracking.​  \n",
    "> 5.​ **Monitoring:** Error drift detection, periodic retraining.​  \n",
    "> 6.​ **Business impact:** Prevent outages, optimize supply, reduce\n",
    "> costs, improve reliability."
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
