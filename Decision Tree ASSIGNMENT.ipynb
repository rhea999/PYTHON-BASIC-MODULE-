{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question 1: What is a Decision Tree, and how does it work in the\n",
    "> context of classification? ANS 1 :- A Decision Tree is a **tree-like\n",
    "> model of decisions**. It breaks down a dataset into smaller and\n",
    "> smaller subsets while at the same time building an associated tree\n",
    "> structure.\n",
    ">\n",
    "> ●​ The **root node** represents the entire dataset.​  \n",
    "> ●​ **Branches** represent the outcomes of a decision (or test on a\n",
    "> feature).​●​ **Leaf nodes** represent the final decision or class label.\n",
    ">\n",
    "> **How it works in Classification:**  \n",
    "> 1.​ **Start at the Root Node**:​  \n",
    "> The algorithm begins with the whole dataset.​  \n",
    "> 2.​ **Splitting Based on Features**:​  \n",
    "> ○​ At each node, the algorithm selects the *best feature* to split the\n",
    "> data.​  \n",
    "> ○​ The \"best\" is decided using metrics like **Gini Index**, **Entropy &\n",
    "> Information** **Gain**, or **Chi-square**.​  \n",
    "> 3.​ **Recursive Partitioning**:​  \n",
    "> The dataset is split into subsets based on the chosen feature, and the\n",
    "> process repeats recursively for each subset.​  \n",
    "> 4.​ **Stopping Criteria**:​  \n",
    "> Splitting stops when one of these occurs:​  \n",
    "> ○​ All samples belong to the same class.​  \n",
    "> ○​ No further information gain can be achieved.​  \n",
    "> ○​ A maximum tree depth is reached.​  \n",
    "> 5.​ **Leaf Node Prediction**:​  \n",
    "> The final leaf node represents the **predicted class** for that subset\n",
    "> of data.​\n",
    ">\n",
    "> **Example (Classification):**  \n",
    "> Suppose we want to predict whether a person will **play tennis** or\n",
    "> not.\n",
    ">\n",
    "> ●​ **Root Node**: Weather (Sunny, Rainy, Overcast)​\n",
    ">\n",
    "> ○​ If **Sunny** → check Humidity (High → \"No\", Normal → \"Yes\")​○​ If\n",
    "> **Overcast** → \"Yes\" (always plays)​  \n",
    "> ○​ If **Rainy** → check Wind (Strong → \"No\", Weak → \"Yes\")​\n",
    ">\n",
    "> Here, the tree makes decisions step by step until it reaches a class\n",
    "> label.\n",
    ">\n",
    "> Question 2: Explain the concepts of Gini Impurity and Entropy as\n",
    "> impurity measures. How do they impact the splits in a Decision Tree?\n",
    ">\n",
    "> ANS 2 :- **Impurity Measures in Decision Trees**  \n",
    "> When building a Decision Tree, the algorithm needs a way to decide\n",
    "> **which feature to split on** at each step.\n",
    ">\n",
    "> ●​ **Impurity measures** (like Gini and Entropy) tell us how “mixed”\n",
    "> the classes are in a node.​  \n",
    "> ●​ A node is **pure** if it contains data points from only one class.​\n",
    ">\n",
    "> **1. Gini Impurity**  \n",
    "> The **Gini Impurity** measures the probability of incorrectly\n",
    "> classifying a randomly chosen element if it was labeled according to\n",
    "> the distribution of labels in that node.\n",
    ">\n",
    "> **Formula:**  \n",
    "> Gini=1−∑pi2  \n",
    "> where:  \n",
    "> ●​ C = number of classes​  \n",
    "> = proportion of samples belonging to class iii​●​ pi​\n",
    ">\n",
    "> ✅**Interpretation:**  \n",
    "> ●​ **Gini = 0** → perfectly pure node (all samples in one class).​●​\n",
    "> **Higher Gini** → more impurity (mixed classes).\n",
    ">\n",
    "> **2. Entropy (Information Gain)**\n",
    ">\n",
    "> Entropy is a measure from **Information Theory** that quantifies the\n",
    "> uncertainty (or randomness) in a node.\n",
    ">\n",
    "> **Formula:**  \n",
    "> Entropy=−∑pilog⁡2(pi)  \n",
    "> ✅**Interpretation:**  \n",
    "> ●​ **Entropy = 0** → perfectly pure node.​\n",
    ">\n",
    "> ●​ **Higher entropy** → more disorder (more mixed classes).​\n",
    ">\n",
    "> To decide a split, Decision Trees often use **Information Gain (IG):**\n",
    "> IG=Entropy(parent)−∑nj/nEntropy(childj)  \n",
    "> where nj is the number of samples in child node j.\n",
    ">\n",
    "> ●​ The split that **maximizes Information Gain** is chosen.\n",
    ">\n",
    "> **3. How They Impact Splits**  \n",
    "> ●​ **Both Gini and Entropy** aim to create the *purest possible child\n",
    "> nodes*.​\n",
    ">\n",
    "> ●​ The chosen split is the one that **reduces impurity the most**.​\n",
    ">\n",
    "> ●​ **Gini** tends to be faster (no log calculations).​\n",
    ">\n",
    "> ●​ **Entropy** is more theoretically grounded (from information\n",
    "> theory).\n",
    ">\n",
    "> **Example**  \n",
    "> Suppose we have 10 samples: 6 belong to Class A, 4 belong to Class B.\n",
    ">\n",
    "> ●​ **Gini:​**\n",
    ">\n",
    "> 1−(0.62+0.42)=1−(0.36+0.16)=0.481 - (0.6^2 + 0.4^2) = 1 - (0.36 +\n",
    "> 0.16) = 0.481−(0.62+0.42)=1−(0.36+0.16)=0.48  \n",
    "> ●​ **Entropy:​**\n",
    ">\n",
    "> −(0.6log⁡20.6+0.4log⁡20.4)≈0.97- (0.6 \\log_2 0.6 + 0.4 \\log_2 0.4)\n",
    "> \\approx 0.97−(0.6log2​0.6+0.4log2​0.4)≈0.97  \n",
    "> Both show the node is **impure**, but Entropy is higher → more\n",
    "> sensitive to imbalance.\n",
    ">\n",
    "> Question 3: What is the difference between Pre-Pruning and\n",
    "> Post-Pruning in Decision Trees? Give one practical advantage of using\n",
    "> each.\n",
    ">\n",
    "> ANS 3 :- **1. Pre-Pruning (Early Stopping)**  \n",
    "> Pre-pruning means **stopping the tree from growing too large** during\n",
    "> training.​  \n",
    "> Instead of letting the tree grow fully and then cutting it back, we\n",
    "> **set constraints upfront**.\n",
    ">\n",
    "> **Examples of pre-pruning methods:**  \n",
    "> ●​ Maximum depth (max_depth)​\n",
    ">\n",
    "> ●​ Minimum number of samples required to split a node\n",
    "> (min_samples_split)​\n",
    ">\n",
    "> ●​ Minimum number of samples per leaf (min_samples_leaf)​\n",
    ">\n",
    "> ●​ Maximum number of leaf nodes (max_leaf_nodes)​\n",
    ">\n",
    "> **Advantage:**  \n",
    "> ●​ **Faster training** because the tree stops growing early.​\n",
    ">\n",
    "> ●​ Helps prevent overfitting before it happens.\n",
    ">\n",
    "> **2. Post-Pruning (Cost Complexity Pruning)**  \n",
    "> Post-pruning means **growing the tree fully first** (allowing\n",
    "> overfitting), then trimming back unnecessary branches.​  \n",
    "> The goal is to simplify the tree while keeping predictive accuracy\n",
    "> high.\n",
    ">\n",
    "> **Common method:**  \n",
    "> ●​ **Cost Complexity Pruning (CCP)**: Remove branches that provide\n",
    "> little improvement in accuracy compared to their complexity penalty.​\n",
    ">\n",
    "> **Advantage:**\n",
    ">\n",
    "> ●​ Usually results in a **more accurate and balanced tree**, since\n",
    "> pruning decisions are based on actual performance (validation data or\n",
    "> cross-validation).\n",
    ">\n",
    "> Question 4: What is Information Gain in Decision Trees, and why is it\n",
    "> important for choosing the best split?\n",
    ">\n",
    "> ANS 4 :-  \n",
    "> **Information Gain (IG)** is a metric used in Decision Trees (when\n",
    "> using **Entropy** as the impurity measure).​  \n",
    "> It tells us **how much “information” (or certainty) we gain about the\n",
    "> target variable** after splitting the dataset on a particular feature.\n",
    ">\n",
    "> In other words:  \n",
    "> ●​ A good split should make the child nodes **purer** than the parent\n",
    "> node.​  \n",
    "> ●​ Information Gain measures **the reduction in uncertainty (entropy)**\n",
    "> after a split.\n",
    ">\n",
    "> **Formula**  \n",
    "> IG(S,A)=Entropy(S)−∑∣Sv∣∣S∣⋅Entropy(Sv) Where:  \n",
    "> ●​ S = current dataset (parent node)​  \n",
    "> ●​ A = attribute (feature) used for splitting​  \n",
    "> ●​ Sv= subset of S for which attribute A has value v​  \n",
    "> ●​ ∣Sv∣/∣S∣ = proportion of samples in subset\n",
    ">\n",
    "> **Why is it Important?**\n",
    ">\n",
    "> ●​ Decision Trees make decisions by asking **yes/no questions** about\n",
    "> features.​  \n",
    "> ●​ To choose the **best split at each node**, we compare IG values for\n",
    "> all features.​●​ The feature with the **highest Information Gain** is\n",
    "> selected, since it reduces uncertainty the most.\n",
    ">\n",
    "> **Example**\n",
    ">\n",
    "> Suppose we want to predict if students **Pass/Fail** based on **Study\n",
    "> Hours**.\n",
    ">\n",
    "> ●​ At the root, the dataset is mixed (50% Pass, 50% Fail) → **Entropy =\n",
    "> 1**.​ ●​ If we split at \"Study Hours \\> 5\":​  \n",
    "> ○​ Left child (Pass) → Entropy = 0 (pure)​  \n",
    "> ○​ Right child (Fail) → Entropy = 0 (pure)​  \n",
    "> ●​ New Entropy = 0 →​  \n",
    "> IG=1−0=1  \n",
    "> This is a perfect split since we gained maximum information.\n",
    ">\n",
    "> Question 5: What are some common real-world applications of Decision\n",
    "> Trees, and what are their main advantages and limitations?\n",
    ">\n",
    "> ANS 5 :- **Real-World Applications of Decision Trees** 1.​\n",
    "> **Healthcare** ​  \n",
    "> ○​ Predicting diseases based on symptoms (e.g., flu vs. cold).​ ○​ Risk\n",
    "> assessment (e.g., likelihood of diabetes).​  \n",
    "> 2.​ **Finance & Banking** ​  \n",
    "> ○​ Credit scoring: deciding whether to approve a loan.​  \n",
    "> ○​ Fraud detection in transactions.​  \n",
    "> 3.​ **Marketing & Sales** ​  \n",
    "> ○​ Customer segmentation (who is likely to buy a product).​  \n",
    "> ○​ Predicting customer churn.​  \n",
    "> 4.​ **Manufacturing & Operations** ​  \n",
    "> ○​ Quality control (classifying defective vs. non-defective products).​\n",
    "> ○​ Supply chain decision-making.​\n",
    ">\n",
    "> 5.​ **Education** ​  \n",
    "> ○​ Predicting student performance (pass/fail, dropout risk).​○​ Adaptive\n",
    "> learning systems (personalized study paths).\n",
    ">\n",
    "> **Advantages of Decision Trees**  \n",
    "> 1.​ **Easy to Understand & Interpret​**  \n",
    "> ○​ Resembles human decision-making with \"if-else\" rules.​  \n",
    "> 2.​ **No Feature Scaling Needed​**  \n",
    "> ○​ Works with raw data (no need for normalization/standardization).​ 3.​\n",
    "> **Handles Both Categorical & Numerical Data​**  \n",
    "> ○​ Flexible with different data types.​  \n",
    "> 4.​ **Non-Parametric​**  \n",
    "> ○​ No assumptions about data distribution (unlike linear regression).​\n",
    "> 5.​ **Fast and Efficient​**  \n",
    "> ○​ Good for small to medium datasets.\n",
    ">\n",
    "> **Limitations of Decision Trees**  \n",
    "> 1.​ **Overfitting** ​  \n",
    "> ○​ Trees can grow too complex and fit noise instead of patterns.​ ○​\n",
    "> (Solved using pruning or ensemble methods like Random Forests).​ 2.​\n",
    "> **Instability​**  \n",
    "> ○​ Small changes in data can lead to a very different tree.​\n",
    ">\n",
    "> 3.​ **Biased with Imbalanced Data​**\n",
    ">\n",
    "> ○​ Trees may favor classes with more samples.​\n",
    ">\n",
    "> 4.​ **Greedy Splitting​**\n",
    ">\n",
    "> ○​ Always chooses the best local split, but not necessarily the global\n",
    "> best tree.​\n",
    ">\n",
    "> 5.​ **Not Great with Continuous Variables Alone​**\n",
    ">\n",
    "> ○​ Can create too many splits, leading to complexity.\n",
    ">\n",
    "> Dataset Info:  \n",
    "> ● Iris Dataset for classification tasks (sklearn.datasets.load_iris()\n",
    "> or provided CSV). ● Boston Housing Dataset for regression tasks\n",
    "> (sklearn.datasets.load_boston() or provided CSV).\n",
    ">\n",
    "> Question 6: Write a Python program to:  \n",
    "> ● Load the Iris Dataset  \n",
    "> ● Train a Decision Tree Classifier using the Gini criterion ● Print\n",
    "> the model’s accuracy and feature importances ANS 6 :-  \n",
    "> from sklearn.datasets import load_iris  \n",
    "> from sklearn.model_selection import train_test_split  \n",
    "> from sklearn.tree import DecisionTreeClassifier  \n",
    "> from sklearn.metrics import accuracy_score  \n",
    "> import pandas as pd\n",
    ">\n",
    "> \\# Load the Iris dataset  \n",
    "> iris = load_iris()  \n",
    "> X = iris.data\n",
    ">\n",
    "> y = iris.target\n",
    ">\n",
    "> \\# Split into train and test sets  \n",
    "> X_train, X_test, y_train, y_test = train_test_split(  \n",
    "> X, y, test_size=0.3, random_state=42  \n",
    "> )\n",
    ">\n",
    "> \\# Train Decision Tree Classifier using Gini criterion  \n",
    "> clf = DecisionTreeClassifier(criterion='gini', random_state=42)  \n",
    "> clf.fit(X_train, y_train)\n",
    ">\n",
    "> \\# Predict on test set  \n",
    "> y_pred = clf.predict(X_test)\n",
    ">\n",
    "> \\# Calculate accuracy  \n",
    "> accuracy = accuracy_score(y_test, y_pred)\n",
    ">\n",
    "> \\# Feature importances  \n",
    "> feature_importances = pd.DataFrame({  \n",
    "> 'Feature': iris.feature_names,  \n",
    "> 'Importance': clf.feature_importances\\_  \n",
    "> }).sort_values(by='Importance', ascending=False)\n",
    ">\n",
    "> print(\"Model Accuracy:\", accuracy)\n",
    ">\n",
    "> print(\"\\nFeature Importances:\\n\", feature_importances)\n",
    ">\n",
    "> **Output**\n",
    ">\n",
    "> ●​ **Model Accuracy:**1.0 (100% on test set)​\n",
    ">\n",
    "> ●​ **Feature Importances:​**  \n",
    "> \\| Feature \\| Importance \\|​  \n",
    "> \\|----------------------\\|------------\\|​  \n",
    "> \\| Petal length (cm) \\| 0.8933 \\|​  \n",
    "> \\| Petal width (cm) \\| 0.0876 \\|​  \n",
    "> \\| Sepal width (cm) \\| 0.0191 \\|​  \n",
    "> \\| Sepal length (cm) \\| 0.0000 \\|\n",
    ">\n",
    "> Question 7: Write a Python program to:\n",
    ">\n",
    "> ● Load the Iris Dataset\n",
    ">\n",
    "> ● Train a Decision Tree Classifier with max_depth=3 and compare its\n",
    "> accuracy to a fully-grown tree.\n",
    ">\n",
    "> ANS 7 :-  \n",
    "> from sklearn.datasets import load_iris  \n",
    "> from sklearn.model_selection import train_test_split  \n",
    "> from sklearn.tree import DecisionTreeClassifier  \n",
    "> from sklearn.metrics import accuracy_score\n",
    ">\n",
    "> \\# Load the Iris dataset  \n",
    "> iris = load_iris()  \n",
    "> X = iris.data  \n",
    "> y = iris.target\n",
    ">\n",
    "> \\# Split dataset  \n",
    "> X_train, X_test, y_train, y_test = train_test_split(  \n",
    "> X, y, test_size=0.3, random_state=42  \n",
    "> )\n",
    ">\n",
    "> \\# Fully grown Decision Tree (no depth limit)  \n",
    "> clf_full = DecisionTreeClassifier(random_state=42)  \n",
    "> clf_full.fit(X_train, y_train)  \n",
    "> y_pred_full = clf_full.predict(X_test)  \n",
    "> accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    ">\n",
    "> \\# Decision Tree with max_depth=3  \n",
    "> clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "> clf_depth3.fit(X_train, y_train)  \n",
    "> y_pred_depth3 = clf_depth3.predict(X_test)  \n",
    "> accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
    ">\n",
    "> print(\"Accuracy of fully-grown tree:\", accuracy_full)  \n",
    "> print(\"Accuracy of tree with max_depth=3:\", accuracy_depth3)\n",
    ">\n",
    "> **Output**\n",
    ">\n",
    "> ●​ **Fully-grown Tree Accuracy:**1.0​\n",
    ">\n",
    "> ●​ **Tree with max_depth=3 Accuracy:**1.0\n",
    ">\n",
    "> **Interpretation:**\n",
    ">\n",
    "> ●​ Both the **fully grown tree** and the **pruned tree (depth=3)**\n",
    "> achieved **100% accuracy** on the Iris test set.​\n",
    ">\n",
    "> ●​ This shows that the Iris dataset is **simple enough** to be\n",
    "> perfectly classified even with a shallow tree.​\n",
    ">\n",
    "> ●​ In larger/noisier datasets, pruning (setting max_depth) usually\n",
    "> helps **reduce overfitting** while maintaining good accuracy.​\n",
    ">\n",
    "> Question 8: Write a Python program to:  \n",
    "> ● Load the California Housing dataset from sklearn  \n",
    "> ● Train a Decision Tree Regressor  \n",
    "> ● Print the Mean Squared Error (MSE) and feature importances\n",
    ">\n",
    "> ANS 8 :-  \n",
    "> from sklearn.datasets import fetch_california_housing  \n",
    "> from sklearn.model_selection import train_test_split  \n",
    "> from sklearn.tree import DecisionTreeRegressor  \n",
    "> from sklearn.metrics import mean_squared_error  \n",
    "> import pandas as pd\n",
    ">\n",
    "> \\# Load California Housing dataset  \n",
    "> california = fetch_california_housing()  \n",
    "> X, y = california.data, california.target\n",
    ">\n",
    "> \\# Split into train and test sets  \n",
    "> X_train, X_test, y_train, y_test = train_test_split(  \n",
    "> X, y, test_size=0.3, random_state=42  \n",
    "> )\n",
    ">\n",
    "> \\# Train Decision Tree Regressor  \n",
    "> regressor = DecisionTreeRegressor(random_state=42)  \n",
    "> regressor.fit(X_train, y_train)\n",
    ">\n",
    "> \\# Predict on test set  \n",
    "> y_pred = regressor.predict(X_test)\n",
    ">\n",
    "> \\# Calculate Mean Squared Error  \n",
    "> mse = mean_squared_error(y_test, y_pred)\n",
    ">\n",
    "> \\# Feature importances  \n",
    "> feature_importances = pd.DataFrame({  \n",
    "> 'Feature': california.feature_names,  \n",
    "> 'Importance': regressor.feature_importances\\_  \n",
    "> }).sort_values(by='Importance', ascending=False)\n",
    ">\n",
    "> print(\"Mean Squared Error (MSE):\", mse)  \n",
    "> print(\"\\nFeature Importances:\\n\", feature_importances)\n",
    ">\n",
    "> **Expected Output (approximate)**\n",
    ">\n",
    "> ●​ **Mean Squared Error (MSE):** around 0.25 – 0.35 (depends on\n",
    "> randomness).​\n",
    ">\n",
    "> ●​ **Feature Importances:​**  \n",
    "> **​**  \n",
    "> Feature Importance  \n",
    "> ●​ 0 MedInc \\~0.55  \n",
    "> ●​ 5 AveOccup \\~0.15  \n",
    "> ●​ 2 AveRooms \\~0.12  \n",
    "> ●​ 7 Latitude \\~0.08  \n",
    "> ●​ 6 AveBedrms \\~0.05  \n",
    "> ●​ 1 HouseAge \\~0.03  \n",
    "> ●​ 3 Population \\~0.02  \n",
    "> ●​ 4 AveOccup \\~0.00\n",
    ">\n",
    "> Question 9: Write a Python program to:\n",
    ">\n",
    "> ● Load the Iris Dataset\n",
    ">\n",
    "> ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
    "> GridSearchCV ● Print the best parameters and the resulting model\n",
    "> accuracy  \n",
    "> ANS 9 :-  \n",
    "> from sklearn.datasets import load_iris  \n",
    "> from sklearn.tree import DecisionTreeClassifier  \n",
    "> from sklearn.model_selection import GridSearchCV\n",
    ">\n",
    "> \\# Load Iris dataset  \n",
    "> iris = load_iris()  \n",
    "> X, y = iris.data, iris.target\n",
    ">\n",
    "> \\# Define parameter grid  \n",
    "> param_grid = {  \n",
    "> 'max_depth': \\[2, 3, 4, 5, None\\],  \n",
    "> 'min_samples_split': \\[2, 3, 4, 5, 6\\]  \n",
    "> }\n",
    ">\n",
    "> \\# Initialize Decision Tree Classifier  \n",
    "> dt = DecisionTreeClassifier(random_state=42)\n",
    ">\n",
    "> \\# GridSearchCV for hyperparameter tuning  \n",
    "> grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')\n",
    "> grid_search.fit(X, y)\n",
    ">\n",
    "> \\# Print best parameters and accuracy  \n",
    "> print(\"Best Parameters:\", grid_search.best_params\\_)  \n",
    "> print(\"Best Cross-Validation Accuracy:\", grid_search.best_score\\_)\n",
    ">\n",
    "> Expected Output (approximate)  \n",
    "> Best Parameters: {'max_depth': 3, 'min_samples_split': 2}  \n",
    "> Best Cross-Validation Accuracy: 0.966\n",
    ">\n",
    "> Question 10: Imagine you’re working as a data scientist for a\n",
    "> healthcare company that wants to predict whether a patient has a\n",
    "> certain disease. You have a large dataset with mixed data types and\n",
    "> some missing values. Explain the step-by-step process you would follow\n",
    "> to:  \n",
    "> ● Handle the missing values  \n",
    "> ● Encode the categorical features  \n",
    "> ● Train a Decision Tree model  \n",
    "> ● Tune its hyperparameters  \n",
    "> ● Evaluate its performance And describe what business value this model\n",
    "> could provide in the real-world setting.\n",
    ">\n",
    "> ANS 10 :-\n",
    ">\n",
    "> **Step-by-Step Approach**\n",
    ">\n",
    "> **1. Handle Missing Values**  \n",
    "> ●​ **Numerical features**: Replace missing values with **mean** or\n",
    "> **median** (robust against outliers).​\n",
    ">\n",
    "> ●​ **Categorical features**: Replace missing values with the **most\n",
    "> frequent category** (mode).​\n",
    ">\n",
    "> ●​ Alternatively, use **KNN imputation** or **iterative imputer** if\n",
    "> the dataset is large and patterns exist in missing data.​  \n",
    "> ●​ Important: Ensure the same imputation strategy is applied to both\n",
    "> **train** and **test** sets.\n",
    ">\n",
    "> **2. Encode the Categorical Features**  \n",
    "> ●​ For categorical variables:​  \n",
    "> ○​ **One-Hot Encoding** for nominal features (e.g., blood type: A, B,\n",
    "> AB, O).​ ○​ **Ordinal Encoding** for features with natural order (e.g.,\n",
    "> disease stage: Stage I \\< Stage II \\< Stage III).​  \n",
    "> ●​ Use **sklearn.preprocessing.OneHotEncoder** or **pd.get_dummies()**.​\n",
    ">\n",
    "> **3. Train a Decision Tree Model**  \n",
    "> ●​ Split data into **training** and **testing** sets (e.g., 80% train,\n",
    "> 20% test).​●​ Initialize a **DecisionTreeClassifier**.​  \n",
    "> ●​ Train the model on the processed data.​\n",
    ">\n",
    "> **4. Tune Hyperparameters**  \n",
    "> ●​ Key hyperparameters:​  \n",
    "> ○​ max_depth → Controls tree depth, prevents overfitting.​  \n",
    "> ○​ min_samples_split → Minimum samples required to split a node.​ ○​\n",
    "> min_samples_leaf → Minimum samples in a leaf node.​  \n",
    "> ○​ criterion → Gini or Entropy.​  \n",
    "> ●​ Use **GridSearchCV** or **RandomizedSearchCV** to find best values.​\n",
    ">\n",
    "> **5. Evaluate Model Performance**\n",
    ">\n",
    "> ●​ Metrics:​\n",
    ">\n",
    "> ○​ **Accuracy**: Overall correctness.​\n",
    ">\n",
    "> ○​ **Precision & Recall**: Especially important in healthcare (false\n",
    "> negatives can be dangerous).​\n",
    ">\n",
    "> ○​ **F1-score**: Balance between precision & recall.​\n",
    ">\n",
    "> ○​ **ROC-AUC**: Measures separability of classes.​\n",
    ">\n",
    "> ●​ Use **cross-validation** for robust evaluation.​\n",
    ">\n",
    "> import pandas as pd  \n",
    "> from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "> from sklearn.tree import DecisionTreeClassifier  \n",
    "> from sklearn.metrics import classification_report, accuracy_score from\n",
    "> sklearn.impute import SimpleImputer  \n",
    "> from sklearn.preprocessing import OneHotEncoder  \n",
    "> from sklearn.compose import ColumnTransformer  \n",
    "> from sklearn.pipeline import Pipeline\n",
    ">\n",
    "> \\# Suppose df is the healthcare dataset  \n",
    "> \\# X = features, y = target (disease: 0 = no, 1 = yes)  \n",
    "> X = df.drop(\"Disease\", axis=1)  \n",
    "> y = df\\[\"Disease\"\\]\n",
    ">\n",
    "> \\# Identify numerical & categorical columns  \n",
    "> num_cols = X.select_dtypes(include=\\[\"int64\", \"float64\"\\]).columns\n",
    "> cat_cols = X.select_dtypes(include=\\[\"object\"\\]).columns\n",
    ">\n",
    "> \\# Preprocessing pipeline  \n",
    "> num_transformer = SimpleImputer(strategy=\"median\")  \n",
    "> cat_transformer = Pipeline(steps=\\[  \n",
    "> ('imputer', SimpleImputer(strategy=\"most_frequent\")),  \n",
    "> ('encoder', OneHotEncoder(handle_unknown='ignore'))  \n",
    "> \\])\n",
    ">\n",
    "> preprocessor = ColumnTransformer(  \n",
    "> transformers=\\[  \n",
    "> ('num', num_transformer, num_cols),  \n",
    "> ('cat', cat_transformer, cat_cols)  \n",
    "> \\])\n",
    ">\n",
    "> \\# Full pipeline with Decision Tree  \n",
    "> pipeline = Pipeline(steps=\\[('preprocessor', preprocessor),  \n",
    "> ('classifier', DecisionTreeClassifier(random_state=42))\\])\n",
    ">\n",
    "> \\# Split data  \n",
    "> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "> test_size=0.2, random_state=42)\n",
    ">\n",
    "> \\# Hyperparameter tuning  \n",
    "> param_grid = {  \n",
    "> 'classifier\\_\\_max_depth': \\[3, 5, 7, None\\],\n",
    ">\n",
    "> 'classifier\\_\\_min_samples_split': \\[2, 5, 10\\],  \n",
    "> 'classifier\\_\\_criterion': \\['gini', 'entropy'\\]  \n",
    "> }  \n",
    "> grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')\n",
    "> grid_search.fit(X_train, y_train)\n",
    ">\n",
    "> \\# Evaluate  \n",
    "> y_pred = grid_search.predict(X_test)  \n",
    "> print(\"Best Parameters:\", grid_search.best_params\\_)  \n",
    "> print(\"Accuracy:\", accuracy_score(y_test, y_pred))  \n",
    "> print(\"\\nClassification Report:\\n\", classification_report(y_test,\n",
    "> y_pred))\n",
    ">\n",
    "> **Business Value in Real-World Healthcare**\n",
    ">\n",
    "> ●​ **Early Disease Detection**: Helps doctors prioritize high-risk\n",
    "> patients.​\n",
    ">\n",
    "> ●​ **Resource Optimization**: Hospitals can allocate resources (tests,\n",
    "> specialists, ICU beds) more efficiently.​\n",
    "\n",
    "●​ **Personalized Care**: Patients flagged as “high risk” can get more\n",
    "frequent checkups.​\n",
    "\n",
    "> ●​ **Cost Reduction**: Avoids unnecessary tests for low-risk patients.​\n",
    ">\n",
    "> ●​ **Explainability**: Decision Trees provide **transparent rules**,\n",
    "> making it easier for doctors to trust AI-driven recommendations."
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
