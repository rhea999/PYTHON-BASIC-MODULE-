{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 1:** What is the difference between K-Means and\n",
    "> Hierarchical Clustering? Provide a use case for each.\n",
    ">\n",
    "> **Answer 1 :-**  \n",
    "> **K-Means Clustering**  \n",
    "> **Definition:**  \n",
    "> ●​ A **partitioning method** that divides data into *K* predefined\n",
    "> clusters.​ ●​ Each cluster is represented by a **centroid** (mean\n",
    "> point).​\n",
    ">\n",
    "> **Key Points:**  \n",
    "> ●​ Requires the number of clusters (*K*) as input.​  \n",
    "> ●​ Works well with large datasets.​  \n",
    "> ●​ Sensitive to initial centroid placement and outliers.​  \n",
    "> ●​ Produces **spherical clusters** (best when clusters are convex and\n",
    "> of similar size).​  \n",
    "> ●​ Fast and efficient (O(n)).​\n",
    ">\n",
    "> **Use Case:**  \n",
    "> ●​ **Customer Segmentation in Marketing​**  \n",
    "> ○​ Group customers based on purchasing behavior into clusters like\n",
    "> “bargain buyers,” “loyal customers,” and “premium spenders.”\n",
    "> **Hierarchical Clustering**  \n",
    "> **Definition:**  \n",
    "> ●​ A **tree-based method** that builds a hierarchy (dendrogram) of\n",
    "> clusters.​\n",
    ">\n",
    "> ●​ Two types:​  \n",
    "> ○​ **Agglomerative** (bottom-up: each point starts as a cluster, then\n",
    "> merges).​  \n",
    "> ○​ **Divisive** (top-down: one big cluster splits into smaller ones).​\n",
    ">\n",
    "> **Key Points:**  \n",
    "> ●​ Doesn’t require specifying number of clusters initially (can be\n",
    "> decided by cutting the dendrogram).​  \n",
    "> ●​ Works well with **small to medium datasets**.​  \n",
    "> ●​ Captures nested clusters and hierarchy.​  \n",
    "> ●​ More computationally expensive (O(n²) or worse).​  \n",
    "> ●​ Not very efficient for very large datasets.​\n",
    ">\n",
    "> **Use Case:**  \n",
    "> ●​ **Gene Expression Analysis in Biology​**  \n",
    "> ○​ Group genes or proteins based on similarity in expression levels to\n",
    "> study relationships between them.\n",
    ">\n",
    "> **Question 2:** Explain the purpose of the Silhouette Score in\n",
    "> evaluating clustering algorithms.\n",
    ">\n",
    "> **Answer 2 :-**  \n",
    "> **Silhouette Score – Purpose**  \n",
    "> The **Silhouette Score** is a metric used to **evaluate the quality of\n",
    "> clusters** formed by a clustering algorithm.\n",
    ">\n",
    "> It measures how well each data point fits within its assigned cluster\n",
    "> compared to other clusters.\n",
    ">\n",
    "> **Formula**\n",
    ">\n",
    "> For each data point *i*:\n",
    ">\n",
    "> ●​ **a(i):** Average distance of point *i* to all other points in the\n",
    "> same cluster (intra-cluster distance).​\n",
    ">\n",
    "> ●​ **b(i):** Minimum average distance of point *i* to all points in\n",
    "> other clusters (nearest-cluster distance).​\n",
    "\n",
    "Silhouette Score(i) = b(i)−a(i)/max⁡(a(i),b(i))\n",
    "\n",
    "> **Interpretation**\n",
    ">\n",
    "> **●​ +1 → Perfect clustering (well-separated, correctly assigned).​**\n",
    ">\n",
    "> **●​ 0 → Point is on/near a cluster boundary.​**\n",
    ">\n",
    "> **●​ -1 → Wrong clustering (closer to another cluster than its own).**\n",
    ">\n",
    "> **Purpose**\n",
    ">\n",
    "> ●​ Helps determine how **cohesive** (tight within clusters) and\n",
    "> **separated** (distinct between clusters) the clusters are.​\n",
    ">\n",
    "> ●​ Provides a way to **compare different clustering algorithms** (e.g.,\n",
    "> K-Means vs Hierarchical).​\n",
    ">\n",
    "> ●​ Helps in choosing the **optimal number of clusters (K)**.​\n",
    ">\n",
    "> **Example**\n",
    ">\n",
    "> ●​ If K=3 gives an average silhouette score of **0.65** and K=5 gives\n",
    "> **0.35**, then K=3 is a better choice.\n",
    ">\n",
    "> **Question 3:** What are the core parameters of DBSCAN, and how do\n",
    "> they influence the clustering process?\n",
    ">\n",
    "> **Answer 3 :-**\n",
    ">\n",
    "> **Core Parameters of DBSCAN**  \n",
    "> DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "> mainly relies on **two parameters**:  \n",
    "> **1. ε (Epsilon / eps)**  \n",
    "> ●​ Defines the **radius of the neighborhood** around a point.​  \n",
    "> ●​ Determines how close points should be to be considered\n",
    "> **neighbors**.​\n",
    ">\n",
    "> **Effect:**  \n",
    "> ●​ **Small ε:** Many small clusters, lots of noise points.​  \n",
    "> ●​ **Large ε:** Fewer, larger clusters (may merge distinct clusters).\n",
    ">\n",
    "> **2. minPts (Minimum Points)**  \n",
    "> ●​ Minimum number of points required within ε-radius for a region to be\n",
    "> considered a **core point**.​  \n",
    "> ●​ Helps define **cluster density**.​\n",
    ">\n",
    "> **Effect:**  \n",
    "> ●​ **Small minPts:** More clusters, risk of noise being treated as\n",
    "> clusters.​●​ **Large minPts:** Fewer clusters, may mark too many points\n",
    "> as noise.\n",
    ">\n",
    "> **How These Influence Clustering**  \n",
    "> ●​ A point is a **core point** if it has ≥ *minPts* within ε.​  \n",
    "> ●​ A point is a **border point** if it’s within ε of a core point but\n",
    "> has \\< *minPts*.​\n",
    ">\n",
    "> ●​ A point is **noise (outlier)** if it’s neither a core nor a border\n",
    "> point.​\n",
    ">\n",
    "> Thus:  \n",
    "> ●​ **ε controls neighborhood size** → affects cluster spread.​\n",
    ">\n",
    "> ●​ **minPts controls density requirement** → affects how strict the\n",
    "> clustering is.\n",
    ">\n",
    "> **Choosing Parameters**  \n",
    "> ●​ **Rule of thumb for minPts:** Usually minPts ≥ dimensions + 1.​\n",
    ">\n",
    "> ●​ **Choosing ε:** Often determined using the **k-distance graph**\n",
    "> (plot distances to k-th nearest neighbor and look for an \"elbow\").\n",
    ">\n",
    "> **Question 4:** Why is feature scaling important when applying\n",
    "> clustering algorithms like K-Means and DBSCAN?\n",
    ">\n",
    "> **Answer 4 :-**\n",
    ">\n",
    "> **Why Feature Scaling Matters in Clustering**\n",
    ">\n",
    "> **1. Distance-Based Nature**  \n",
    "> ●​ Both **K-Means** and **DBSCAN** rely on **distance metrics**\n",
    "> (usually Euclidean distance) to measure similarity.​\n",
    ">\n",
    "> ●​ If features are on different scales, the feature with the **largest\n",
    "> range** dominates the distance calculation.​\n",
    ">\n",
    "> Example:  \n",
    "> ●​ Feature 1: Age (20–60)​\n",
    ">\n",
    "> ●​ Feature 2: Income (20,000–200,000)​\n",
    ">\n",
    "> ●​ Income will overshadow Age in distance calculation unless scaled.​\n",
    ">\n",
    "> **2. K-Means Impact**  \n",
    "> ●​ K-Means assigns points to the nearest centroid.​  \n",
    "> ●​ Without scaling, centroids will shift heavily toward high-range\n",
    "> features.​●​ Scaling ensures **all features contribute equally**.​\n",
    ">\n",
    "> **3. DBSCAN Impact**  \n",
    "> ●​ DBSCAN uses **ε (neighborhood radius)** to determine clusters.​  \n",
    "> ●​ If features are not scaled, ε may be too small/large in certain\n",
    "> dimensions, leading to:​  \n",
    "> ○​ Wrong cluster formation​  \n",
    "> ○​ More noise points​  \n",
    "> ○​ Missed clusters​\n",
    ">\n",
    "> **4. Ensures Fair Feature Contribution**  \n",
    "> ●​ Scaling prevents bias toward features with larger units (e.g.,\n",
    "> kilometers vs meters, dollars vs percentages).​  \n",
    "> ●​ Allows clustering to reflect **true similarity** rather than\n",
    "> measurement scale.​\n",
    ">\n",
    "> **Common Scaling Techniques**\n",
    ">\n",
    "> ●​ **Standardization (Z-score):​**  \n",
    "> Z = x−μ/ σ​  \n",
    "> → mean = 0, std = 1.​\n",
    ">\n",
    "> ●​ **Min-Max Normalization:​**  \n",
    "> x′= x−xmin / ⁡xmax ⁡− x min⁡  \n",
    "> → values between 0 and 1.\n",
    ">\n",
    "> **Question 5:** What is the Elbow Method in K-Means clustering and how\n",
    "> does it help determine the optimal number of clusters?\n",
    ">\n",
    "> **Answer 5 :-**\n",
    ">\n",
    "> **Elbow Method in K-Means**\n",
    ">\n",
    "> **1. What It Is**\n",
    ">\n",
    "> ●​ The **Elbow Method** is a technique used to determine the **optimal\n",
    "> number** **of clusters (K)** in K-Means.​\n",
    ">\n",
    "> ●​ It’s based on analyzing the **Within-Cluster Sum of Squares\n",
    "> (WCSS)**, also called **inertia**.​\n",
    "\n",
    "WCSS= ∑k=1 K ∑x∈Ck∣∣x−μk∣∣2\n",
    "\n",
    "> (where CkC_kCk​ is cluster kkk, and μk\\mu_kμk​ is its centroid).\n",
    ">\n",
    "> **2. How It Works**\n",
    ">\n",
    "> 1.​ Run K-Means with different values of KKK (e.g., 1 to 10).​\n",
    ">\n",
    "> 2.​ Compute the **WCSS (inertia)** for each KKK.​\n",
    ">\n",
    "> 3.​ Plot **WCSS vs K**.​\n",
    ">\n",
    "> 4.​ Look for a point where the WCSS starts to **decrease slowly** —\n",
    "> this point looks like an **“elbow”** in the curve.​\n",
    ">\n",
    "> **3. Interpretation**\n",
    ">\n",
    "> ●​ **Before the elbow:** Adding clusters reduces WCSS significantly\n",
    "> (better clustering).​\n",
    ">\n",
    "> ●​ **After the elbow:** Adding clusters gives **diminishing returns**\n",
    "> (not much improvement).​\n",
    ">\n",
    "> ●​ The **elbow point** suggests the **optimal K**.​\n",
    ">\n",
    "> **4. Why It’s Useful**\n",
    ">\n",
    "> ●​ Prevents **under-clustering** (too few clusters) and\n",
    "> **over-clustering** (too many clusters).​\n",
    ">\n",
    "> ●​ Provides a **visual, intuitive way** to select KKK.\n",
    ">\n",
    "> **Dataset: Use make_blobs, make_moons, and\n",
    "> sklearn.datasets.load_wine() as specified.**\n",
    ">\n",
    "> **Question 6:** Generate synthetic data usingmake_blobs(n_samples=300,\n",
    "> centers=4), apply KMeans clustering, and visualize the results with\n",
    "> cluster centers.\n",
    ">\n",
    "> **Answer 6 :-**  \n",
    "> import matplotlib.pyplot as plt  \n",
    "> from sklearn.datasets import make_blobs  \n",
    "> from sklearn.cluster import KMeans  \n",
    "> import numpy as np\n",
    ">\n",
    "> \\# Step 1: Generate synthetic data  \n",
    "> X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "> cluster_std=0.60,random_state=42)\n",
    ">\n",
    "> \\# Step 2: Apply KMeans clustering  \n",
    "> kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)y_kmeans =\n",
    "> kmeans.fit_predict(X)\n",
    ">\n",
    "> \\# Step 3: Get cluster centers  \n",
    "> centroids = np.round(kmeans.cluster_centers\\_, 2)\n",
    ">\n",
    "> \\# Step 4: Visualize results with cluster centers  \n",
    "> plt.figure(figsize=(8,6))  \n",
    "> plt.scatter(X\\[:, 0\\], X\\[:, 1\\], c=y_kmeans, s=30, cmap='viridis')  \n",
    "> plt.scatter(kmeans.cluster_centers\\_\\[:, 0\\],\n",
    "> kmeans.cluster_centers\\_\\[:, 1\\],\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>c='red', marker='X', s=200, label='Centroids')</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> plt.title(\"KMeans Clustering on make_blobs Data\")  \n",
    "> plt.legend()  \n",
    "> plt.show()\n",
    ">\n",
    "> print(\"Cluster Centers (rounded):\")  \n",
    "> print(centroids)\n",
    ">\n",
    "> **Output**\n",
    ">\n",
    "> **●​ Cluster centers:**\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 50%\" />\n",
    "<col style=\"width: 50%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>[[ 4.69 ​ [-2.61 ​ [-6.85 ​ [-8.83​</p>\n",
    "</blockquote></th>\n",
    "<th><blockquote>\n",
    "<p>2.01]<br />\n",
    "8.99]<br />\n",
    "-6.85]<br />\n",
    "7.24]]</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> ●​ **Visualization:​**  \n",
    "> The scatterplot shows **4 clusters** in different colors, with **red X\n",
    "> markers** representing the centroids.\n",
    ">\n",
    "> **Question 7:** Load the Wine dataset, apply StandardScaler , and then\n",
    "> train a DBSCAN model. Print the number of clusters found (excluding\n",
    "> noise).\n",
    ">\n",
    "> **Answer 7 :-**  \n",
    "> from sklearn.datasets import load_wine  \n",
    "> from sklearn.preprocessing import StandardScaler  \n",
    "> from sklearn.cluster import DBSCAN  \n",
    "> import numpy as np\n",
    ">\n",
    "> \\# Step 1: Load the Wine dataset  \n",
    "> wine = load_wine()  \n",
    "> X = wine.data\n",
    ">\n",
    "> \\# Step 2: Apply StandardScaler  \n",
    "> scaler = StandardScaler()  \n",
    "> X_scaled = scaler.fit_transform(X)\n",
    ">\n",
    "> \\# Step 3: Train DBSCAN model  \n",
    "> dbscan = DBSCAN(eps=1.8, min_samples=6) \\# tuned parametersy_dbscan =\n",
    "> dbscan.fit_predict(X_scaled)\n",
    ">\n",
    "> \\# Step 4: Count clusters (excluding noise)  \n",
    "> n_clusters = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)\n",
    ">\n",
    "> print(\"Number of clusters found (excluding noise):\", n_clusters)\n",
    ">\n",
    "> **Output**  \n",
    "> Number of clusters found (excluding noise): 6\n",
    ">\n",
    "> **Question 8:** Generate moon-shaped synthetic data using  \n",
    "> make_moons(n_samples=200, noise=0.1), apply DBSCAN, and highlight the\n",
    "> outliers in the plot.\n",
    ">\n",
    "> **Answer 8 :-**  \n",
    "> import matplotlib.pyplot as plt  \n",
    "> from sklearn.datasets import make_moons  \n",
    "> from sklearn.cluster import DBSCAN\n",
    ">\n",
    "> \\# Step 1: Generate moon-shaped synthetic data  \n",
    "> X_moons, y_moons = make_moons(n_samples=200,\n",
    "> noise=0.1,random_state=42)\n",
    ">\n",
    "> \\# Step 2: Apply DBSCAN  \n",
    "> dbscan_moons = DBSCAN(eps=0.3, min_samples=5) \\# tune eps\n",
    "> andmin_samples  \n",
    "> y_dbscan_moons = dbscan_moons.fit_predict(X_moons)\n",
    ">\n",
    "> \\# Step 3: Visualization  \n",
    "> plt.figure(figsize=(8,6))\n",
    ">\n",
    "> \\# Plot clustered points  \n",
    "> plt.scatter(X_moons\\[y_dbscan_moons \\>= 0, 0\\],\n",
    "> X_moons\\[y_dbscan_moons \\>=0, 1\\],\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>c=y_dbscan_moons[y_dbscan_moons &gt;= 0], cmap='plasma', s=30,</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> label=\"Clusters\")\n",
    ">\n",
    "> \\# Plot outliers (noise points = -1) in black with 'x'  \n",
    "> plt.scatter(X_moons\\[y_dbscan_moons == -1, 0\\],\n",
    "> X_moons\\[y_dbscan_moons ==-1, 1\\],\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>c='black', marker='x', s=60, label=\"Outliers\")</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> plt.title(\"DBSCAN on Moon-shaped Data (make_moons)\")plt.legend()  \n",
    "> plt.show()\n",
    ">\n",
    "> \\# Step 4: Print results  \n",
    "> n_clusters = len(set(y_dbscan_moons)) - (1 if -1 in y_dbscan_moons\n",
    "> else 0)n_noise = list(y_dbscan_moons).count(-1)\n",
    ">\n",
    "> print(\"Number of clusters found (excluding noise):\",\n",
    "> n_clusters)print(\"Number of outliers (noise points):\", n_noise)\n",
    ">\n",
    "> **Expected Output**  \n",
    "> ●​ **Plot:​**\n",
    ">\n",
    "> ○​ Two moon-shaped clusters (different colors).​\n",
    ">\n",
    "> ○​ Outliers (noise points) marked as **black ‘x’**.​\n",
    ">\n",
    "> ●​ **Printed Results (example):**  \n",
    "> Number of clusters found (excluding noise): 2  \n",
    "> Number of outliers (noise points): 4\n",
    ">\n",
    "> **Question 9:** Load the Wine dataset, reduce it to 2D using PCA, then\n",
    "> apply Agglomerative Clustering and visualize the result in 2D with a\n",
    "> scatter plot.\n",
    ">\n",
    "> **Answer 9 :-**  \n",
    "> import matplotlib.pyplot as plt  \n",
    "> from sklearn.datasets import load_wine  \n",
    "> from sklearn.preprocessing import StandardScaler  \n",
    "> from sklearn.decomposition import PCA  \n",
    "> from sklearn.cluster import AgglomerativeClustering\n",
    ">\n",
    "> \\# Step 1: Load Wine dataset  \n",
    "> wine = load_wine()  \n",
    "> X = wine.data\n",
    ">\n",
    "> \\# Step 2: Standardize the features  \n",
    "> scaler = StandardScaler()  \n",
    "> X_scaled = scaler.fit_transform(X)\n",
    ">\n",
    "> \\# Step 3: Reduce to 2D using PCA  \n",
    "> pca = PCA(n_components=2)  \n",
    "> X_pca = pca.fit_transform(X_scaled)\n",
    ">\n",
    "> \\# Step 4: Apply Agglomerative Clustering  \n",
    "> agg = AgglomerativeClustering(n_clusters=3) \\# wine has 3 classes\n",
    "> y_agg = agg.fit_predict(X_pca)\n",
    ">\n",
    "> \\# Step 5: Visualization  \n",
    "> plt.figure(figsize=(8,6))  \n",
    "> plt.scatter(X_pca\\[:, 0\\], X_pca\\[:, 1\\], c=y_agg, cmap=\"viridis\",\n",
    "> s=40) plt.title(\"Agglomerative Clustering on Wine Dataset (2D PCA)\")\n",
    "> plt.xlabel(\"PCA Component 1\")  \n",
    "> plt.ylabel(\"PCA Component 2\")  \n",
    "> plt.show()\n",
    ">\n",
    "> **Expected Output**\n",
    ">\n",
    "> ●​ **Scatter Plot:​**\n",
    ">\n",
    "> ○​ Data points plotted in **2D PCA space**.​\n",
    ">\n",
    "> ○​ Colors represent **clusters found by Agglomerative Clustering**.​\n",
    ">\n",
    "> ●​ Since the Wine dataset has **3 natural classes**, setting\n",
    "> n_clusters=3 usually produces **three distinct groups** in the plot.\n",
    ">\n",
    "> **Question 10:** You are working as a data analyst at an e-commerce\n",
    "> company. The marketing team wants to segment customers based on their\n",
    "> purchasing behavior to run targeted promotions. The dataset contains\n",
    "> customer  \n",
    "> demographics and their product purchase history across categories.\n",
    "> Describe your real-world data science workflow using clustering:  \n",
    "> ● Which clustering algorithm(s) would you use and why?\n",
    ">\n",
    "> ● How would you preprocess the data (missing values, scaling)?\n",
    ">\n",
    "> ● How would you determine the number of clusters?\n",
    ">\n",
    "> ● How would the marketing team benefit from your clustering analysis?\n",
    ">\n",
    "> **Answer 10 :-**\n",
    ">\n",
    "> **Clustering Workflow for Customer Segmentation**\n",
    ">\n",
    "> **1. Choice of Clustering Algorithm**\n",
    ">\n",
    "> **●​ K-Means:​**\n",
    ">\n",
    "> **○​ Works well for large datasets, efficient, and easy to interpret.​**\n",
    ">\n",
    "> **○​ Suitable if we expect spherical, well-separated clusters.​**\n",
    ">\n",
    "> **●​ DBSCAN (alternative):​**\n",
    ">\n",
    "> **○​ Useful if customer groups are of varying density or contain**\n",
    "> **noise (outliers).​**\n",
    ">\n",
    "> **●​ Hierarchical Clustering (exploration):​**\n",
    ">\n",
    "> **○​ Can be used initially to explore natural cluster structures\n",
    "> before** **choosing final K.​**\n",
    ">\n",
    "> **Final Choice:**  \n",
    "> **●​ Start with K-Means for scalability and interpretability.​●​ Use\n",
    "> Hierarchical for exploration and validation.​**\n",
    ">\n",
    "> **2. Data Preprocessing**  \n",
    "> **●​ Handle Missing Values:​**  \n",
    "> **○​ Numerical: impute with mean/median.​**  \n",
    "> **○​ Categorical: impute with mode or add a “missing” category.​** **●​\n",
    "> Feature Scaling:​**  \n",
    "> **○​ Apply StandardScaler or Min-Max Scaling so that features like**\n",
    "> **“Age (20–70)” and “Annual Spending (₹1000–₹10,00,000)”**\n",
    "> **contribute equally.​**  \n",
    "> **●​ Encoding Categorical Features:​**  \n",
    "> **○​ Use One-Hot Encoding for customer demographics (e.g.,** **gender,\n",
    "> region).​**  \n",
    "> **●​ Feature Engineering:​**  \n",
    "> **○​ Aggregate purchase behavior into features like average spend**\n",
    "> **per category, purchase frequency, recency of last purchase** **(RFM\n",
    "> features).​**\n",
    ">\n",
    "> **3. Determining Number of Clusters**\n",
    ">\n",
    "> **●​ Elbow Method: Plot inertia (WCSS) vs K, look for elbow point.​●​\n",
    "> Silhouette Score: Choose K with higher average silhouette score.​●​\n",
    "> Domain Knowledge: Marketing may want 3–6 actionable segments,** **not\n",
    "> 20 tiny ones.​**\n",
    ">\n",
    "> **4. Benefits to Marketing Team**  \n",
    "> **●​ Targeted Promotions:​**  \n",
    "> **○​ High spenders → premium offers.​**  \n",
    "> **○​ Discount shoppers → coupon-based promotions.​**  \n",
    "> **○​ Loyal customers → retention campaigns.​**  \n",
    "> **●​ Product Recommendations:​**  \n",
    "> **○​ Cluster insights can reveal product affinities for personalized**\n",
    "> **recommendations.​**  \n",
    "> **●​ Improved ROI:​**  \n",
    "> **○​ Focus marketing budget on clusters with higher conversion**\n",
    "> **potential.​**  \n",
    "> **●​ Customer Insights:​**  \n",
    "> **○​ Understand demographic + behavioral patterns (e.g., young**\n",
    "> **professionals buying gadgets vs families buying household**\n",
    "> **items).**\n",
    ">\n",
    "> **Python code :-**  \n",
    "> import pandas as pd\n",
    ">\n",
    "> import numpy as np  \n",
    "> from sklearn.preprocessing import StandardScaler  \n",
    "> from sklearn.impute import SimpleImputer  \n",
    "> from sklearn.cluster import KMeans  \n",
    "> from sklearn.metrics import silhouette_score  \n",
    "> import matplotlib.pyplot as plt\n",
    ">\n",
    "> \\# -------------------------------  \n",
    "> \\# Step 1: Simulate a dataset  \n",
    "> \\# -------------------------------  \n",
    "> np.random.seed(42)  \n",
    "> data = {\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"Age\": np.random.randint(18, 65, 200),</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"Annual_Income\": np.random.randint(20000, 150000, 200),</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"Spending_Score\": np.random.randint(1, 100, 200), # proxy for\n",
    "purchase</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> behavior\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"Electronics_Spend\": np.random.randint(0, 5000, 200),</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"Clothing_Spend\": np.random.randint(0, 5000, 200),</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"Grocery_Spend\": np.random.randint(0, 5000, 200)</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> }  \n",
    "> df = pd.DataFrame(data)\n",
    ">\n",
    "> print(\"Sample of dataset:\")  \n",
    "> print(df.head())\n",
    ">\n",
    "> \\# -------------------------------  \n",
    "> \\# Step 2: Handle missing values (if any)  \n",
    "> \\# -------------------------------  \n",
    "> imputer = SimpleImputer(strategy=\"median\")  \n",
    "> X_imputed = imputer.fit_transform(df)\n",
    ">\n",
    "> \\# -------------------------------  \n",
    "> \\# Step 3: Scale features  \n",
    "> \\# -------------------------------  \n",
    "> scaler = StandardScaler()  \n",
    "> X_scaled = scaler.fit_transform(X_imputed)\n",
    ">\n",
    "> \\# -------------------------------  \n",
    "> \\# Step 4: Find optimal number of clusters using Elbow + Silhouette#\n",
    "> -------------------------------  \n",
    "> wcss = \\[\\]  \n",
    "> silhouette_scores = \\[\\]  \n",
    "> K_range = range(2, 8)\n",
    ">\n",
    "> for k in K_range:\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>labels = kmeans.fit_predict(X_scaled)</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>wcss.append(kmeans.inertia_)</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>silhouette_scores.append(silhouette_score(X_scaled, labels))</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> \\# Plot Elbow Method  \n",
    "> plt.figure(figsize=(12,5))\n",
    ">\n",
    "> plt.subplot(1,2,1)  \n",
    "> plt.plot(K_range, wcss, marker='o')  \n",
    "> plt.title(\"Elbow Method (WCSS vs K)\")  \n",
    "> plt.xlabel(\"Number of Clusters\")  \n",
    "> plt.ylabel(\"WCSS\")\n",
    ">\n",
    "> \\# Plot Silhouette Scores  \n",
    "> plt.subplot(1,2,2)  \n",
    "> plt.plot(K_range, silhouette_scores, marker='o')  \n",
    "> plt.title(\"Silhouette Score vs K\")  \n",
    "> plt.xlabel(\"Number of Clusters\")  \n",
    "> plt.ylabel(\"Silhouette Score\")\n",
    ">\n",
    "> plt.tight_layout()  \n",
    "> plt.show()\n",
    ">\n",
    "> \\# -------------------------------  \n",
    "> \\# Step 5: Train final KMeans model  \n",
    "> \\# -------------------------------  \n",
    "> best_k = 4 \\# assume from elbow/silhouette  \n",
    "> kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    ">\n",
    "> df\\[\"Cluster\"\\] = kmeans.fit_predict(X_scaled)\n",
    ">\n",
    "> print(\"\\nCluster distribution:\")  \n",
    "> print(df\\[\"Cluster\"\\].value_counts())\n",
    ">\n",
    "> print(\"\\nCluster centroids (scaled space):\")  \n",
    "> print(kmeans.cluster_centers\\_)\n",
    ">\n",
    "> **Output**  \n",
    "> Sample of dataset:  \n",
    "> Age Annual_Income Spending_Score Electronics_Spend Clothing_Spend0 56\n",
    "> 72733 93 2896 20481 46 85318 46 4185 41462 32 129953 6 775 47803 60\n",
    "> 109474 99 1625 7164 25 43664 37 3069 4544 Grocery_Spend  \n",
    "> 0 9  \n",
    "> 1 260  \n",
    "> 2 1673  \n",
    "> 3 2219  \n",
    "> 4 2060\n",
    ">\n",
    "> Cluster distribution:  \n",
    "> Cluster  \n",
    "> 0 55  \n",
    "> 1 54  \n",
    "> 3 49  \n",
    "> 2 42  \n",
    "> Name: count, dtype: int64\n",
    ">\n",
    "> Cluster centroids (scaled space):\n",
    ">\n",
    "> \\[\\[ 0.56168671 0.37082543 0.99612086 -0.3105289 -0.19813552\n",
    "> 0.37980593\\] \\[-0.33538872 -0.99932247 -0.67323957 -0.27642637\n",
    "> -0.13495824 0.5137487 \\] \\[ 0.59006959 -0.18464394 -0.05517582\n",
    "> 0.79110238 -0.27984004 -1.09766207\\] \\[-0.76662654 0.84332776\n",
    "> -0.3288638 -0.02490177 0.61098939  \n",
    "> -0.05163161\\]\\]"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
