{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 1:** What is K-Nearest Neighbors (KNN) and how does it work\n",
    "> in both classification and regression problems?\n",
    ">\n",
    "> **Answer 1 :-**\n",
    ">\n",
    "> **K-Nearest Neighbors (KNN)**\n",
    ">\n",
    "> K-Nearest Neighbors (KNN) is a supervised machine learning algorithm\n",
    "> that can be used for both classification and regression problems.​  \n",
    "> It is called a “lazy learner” because it doesn’t build an explicit\n",
    "> model during training—instead, it stores all the training data and\n",
    "> makes predictions only when a new data point needs to be classified or\n",
    "> predicted.\n",
    ">\n",
    "> **How KNN Works**\n",
    ">\n",
    "> 1.​ Choose a value of K (the number of nearest neighbors to consider).​\n",
    ">\n",
    "> 2.​ For a new data point:​\n",
    ">\n",
    "> ○​ Calculate the distance (commonly Euclidean distance) between this\n",
    "> point and all the training points.​\n",
    ">\n",
    "> ○​ Identify the K closest neighbors.​\n",
    ">\n",
    "> **3.​** Make a prediction based on these neighbors.**​**\n",
    ">\n",
    "> **KNN for Classification**\n",
    ">\n",
    "> ●​ Each of the K nearest neighbors \"votes\" for its class.​\n",
    ">\n",
    "> ●​ The class with the **majority votes** is assigned to the new data\n",
    "> point.​\n",
    ">\n",
    "> Example:​  \n",
    "> If K = 5 and among the 5 neighbors:\n",
    ">\n",
    "> ●​ 3 belong to class “A”​\n",
    ">\n",
    "> ●​ 2 belong to class “B”​  \n",
    "> → The new point is classified as **class A**.\n",
    ">\n",
    "> **KNN for Regression**\n",
    ">\n",
    "> ●​ Instead of voting for classes, KNN takes the **average (or\n",
    "> weighted** **average)** of the target values of the K nearest\n",
    "> neighbors.​\n",
    ">\n",
    "> ●​ The predicted value is continuous.​\n",
    ">\n",
    "> Example:​  \n",
    "> If K = 3 and the neighbor values are \\[50, 60, 70\\],​ → Prediction =\n",
    "> (50 + 60 + 70) / 3 = **60**.\n",
    ">\n",
    "> **Question 2:** What is the Curse of Dimensionality and how does it\n",
    "> affect KNN performance?\n",
    ">\n",
    "> **Answer 2 :-**\n",
    ">\n",
    "> **Curse of Dimensionality**\n",
    ">\n",
    "> The **Curse of Dimensionality** refers to various problems that arise\n",
    "> when working with data in **high-dimensional spaces** (i.e., when the\n",
    "> number of features/variables is very large).\n",
    ">\n",
    "> In high dimensions:\n",
    ">\n",
    "> ●​ Data becomes sparse.​\n",
    ">\n",
    "> ●​ Distance measures (like Euclidean distance) lose meaning.​\n",
    ">\n",
    "> ●​ Models like **KNN**, which rely on distance, struggle to find “true”\n",
    "> neighbors.​  \n",
    "> **How it Affects KNN**\n",
    ">\n",
    "> KNN works by finding the **nearest neighbors** using distance metrics.\n",
    "> But in high dimensions:  \n",
    "> 1.​ **Distances become less informative​**  \n",
    "> ○​ In low dimensions, the nearest neighbor is clearly closer than\n",
    "> others.​  \n",
    "> ○​ In high dimensions, the difference between the nearest and farthest\n",
    "> points becomes very small → all points look “almost equally far.”​  \n",
    "> ○​ This makes it hard for KNN to distinguish which points are actually\n",
    "> neighbors.​  \n",
    "> 2.​ **Sparsity of data​**  \n",
    "> ○​ With many features, data points are spread thinly across the\n",
    "> space.​  \n",
    "> ○​ A chosen “neighbor” may not really be similar in meaningful ways.​  \n",
    "> 3.​ **Increased computational cost​**  \n",
    "> ○​ KNN needs to compute distances to all points.​  \n",
    "> ○​ Higher dimensions = more computations, making KNN very slow.​  \n",
    "> 4.​ **Risk of overfitting​**  \n",
    "> ●​ When dimensions are large, KNN may overfit because it tries to fit\n",
    "> to sparse data with little overlap.​\n",
    ">\n",
    "> **Example (Intuition)**\n",
    ">\n",
    "> ●​ In **2D** (say, height & weight), you can easily find similar\n",
    "> people.​\n",
    ">\n",
    "> ●​ In **100D** (lots of random features), even the “closest” neighbor\n",
    "> may not really be similar—it might just be close due to noise.​\n",
    ">\n",
    "> **Ways to Handle Curse of Dimensionality in KNN**\n",
    ">\n",
    "> ●​ **Feature Selection** → keep only the most important features.​\n",
    ">\n",
    "> ●​ **Dimensionality Reduction** (PCA, t-SNE, Autoencoders).​\n",
    ">\n",
    "> ●​ **Scaling/Normalization** → ensures fair contribution of features.​\n",
    ">\n",
    "> **Question 3:** What is Principal Component Analysis (PCA)? How is it\n",
    "> different from feature selection?\n",
    ">\n",
    "> **Answer 3 :-**\n",
    ">\n",
    "> **Principal Component Analysis (PCA)**\n",
    ">\n",
    "> PCA is a **dimensionality reduction technique** used in machine\n",
    "> learning and statistics.​  \n",
    "> Its main goal is to transform high-dimensional data into a  \n",
    "> lower-dimensional space while retaining as much **variance\n",
    "> (information)** as possible.\n",
    ">\n",
    "> **How PCA Works (Conceptually)**\n",
    ">\n",
    "> 1.​ **Standardize the data** (so all features contribute equally).​\n",
    ">\n",
    "> 2.​ **Compute covariance matrix** of the features.​\n",
    ">\n",
    "> 3.​ **Find eigenvectors and eigenvalues** of the covariance matrix.​ ○​\n",
    "> Eigenvectors = new directions (called **principal components**)​ ○​\n",
    "> Eigenvalues = amount of variance explained by each  \n",
    "> component​  \n",
    "> 4.​ **Select top k principal components** (those with highest\n",
    "> variance).​5.​ **Project data** onto this new lower-dimensional space.​\n",
    ">\n",
    "> Example: If you have 100 features, PCA may reduce them to 2 or 3\n",
    "> “principal components” that still capture most of the variability in\n",
    "> the data.\n",
    ">\n",
    "> **PCA vs Feature Selection**  \n",
    "> 1.​ **Definition​**  \n",
    "> ○​ **PCA**: A **dimensionality reduction** technique that creates new\n",
    "> features (principal components) as linear combinations of the original\n",
    "> features.​  \n",
    "> ○​ **Feature Selection**: A process of **choosing a subset** of the\n",
    "> original features and discarding the rest.​  \n",
    "> 2.​ **Nature of Features​**  \n",
    "> ○​ **PCA**: Produces **new transformed features** (PC1, PC2, …).​ ○​\n",
    "> **Feature Selection**: Keeps **original features** as they are.​ 3.​\n",
    "> **Interpretability​**\n",
    ">\n",
    "> ○​ **PCA**: Difficult to interpret, since new features are combinations\n",
    "> of many variables.​  \n",
    "> ○​ **Feature Selection**: Easy to interpret, since it uses original\n",
    "> features.​  \n",
    "> 4.​ **Objective​**  \n",
    "> ○​ **PCA**: To capture maximum **variance (information)** with fewer\n",
    "> dimensions.​  \n",
    "> ○​ **Feature Selection**: To keep the most **relevant/important**\n",
    "> features and remove redundant or noisy ones.​  \n",
    "> 5.​ **Techniques Used​**  \n",
    "> ○​ **PCA**: Uses **linear algebra** (eigenvalues, eigenvectors,  \n",
    "> covariance matrix).​  \n",
    "> ○​ **Feature Selection**: Uses **filter methods** (correlation,  \n",
    "> chi-square), **wrapper methods** (forward/backward selection), or\n",
    "> **embedded methods** (LASSO, tree-based importance).​6.​ **When to\n",
    "> Use​**  \n",
    "> ○​ **PCA**: When data has many correlated features and reducing\n",
    "> dimensionality is important.​  \n",
    "> ○​ **Feature Selection**: When interpretability is crucial and we want\n",
    "> to remove irrelevant/noisy variables.​\n",
    ">\n",
    "> **Question 4:** What are eigenvalues and eigenvectors in PCA, and why\n",
    "> are they important?\n",
    ">\n",
    "> **Eigenvalues and Eigenvectors in PCA**\n",
    ">\n",
    "> In Principal Component Analysis (PCA), the **covariance matrix** of\n",
    "> the dataset is decomposed into its **eigenvectors** and\n",
    "> **eigenvalues**.\n",
    ">\n",
    "> ●​ **Eigenvectors**: These represent the **directions** (principal  \n",
    "> components) along which the data varies the most. Each eigenvector is\n",
    "> a new axis in the transformed feature space.​\n",
    ">\n",
    "> ●​ **Eigenvalues**: These represent the **magnitude of variance**\n",
    "> captured by their corresponding eigenvectors. A larger eigenvalue\n",
    "> means that the corresponding eigenvector explains more of the data’s\n",
    "> spread.\n",
    ">\n",
    "> **Why They Are Important in PCA**\n",
    ">\n",
    "> 1.​ **Identify principal components​**\n",
    ">\n",
    "> ○​ Eigenvectors define the new coordinate system where the axes are\n",
    "> aligned with maximum variance.​\n",
    ">\n",
    "> 2.​ **Rank components by importance​**\n",
    ">\n",
    "> ○​ Eigenvalues tell us how much information (variance) each principal\n",
    "> component contains. Components with higher eigenvalues are more\n",
    "> important.​\n",
    ">\n",
    "> 3.​ **Dimensionality reduction​**\n",
    ">\n",
    "> ○​ By selecting the top-k eigenvectors (with the largest  \n",
    "> eigenvalues), PCA reduces the number of dimensions while preserving\n",
    "> most of the data’s variance.\n",
    ">\n",
    "> **Question 5:** How do KNN and PCA complement each other when applied\n",
    "> in a single pipeline?\n",
    ">\n",
    "> **Answer 5 :-**\n",
    ">\n",
    "> **How KNN and PCA Complement Each Other**  \n",
    "> 1.​ **KNN’s Limitation: Curse of Dimensionality​**  \n",
    "> ○​ KNN relies on **distance metrics** (Euclidean, Manhattan, etc.) to\n",
    "> find nearest neighbors.​  \n",
    "> ○​ In **high-dimensional data**, distances lose meaning (all points\n",
    "> appear equally far), which hurts KNN performance.​  \n",
    "> 2.​ **PCA’s Role: Dimensionality Reduction​**  \n",
    "> ○​ PCA reduces the number of dimensions by projecting data into a\n",
    "> lower-dimensional space.​  \n",
    "> ○​ It keeps the most important variance while removing  \n",
    "> redundancy and noise.​  \n",
    "> 3.​ **Combined Effect in a Pipeline​**  \n",
    "> ○​ By applying **PCA before KNN**:​  \n",
    "> ■​ Distances between points become more meaningful.​ ■​ Noise and\n",
    "> irrelevant features are removed.​  \n",
    "> ■​ Computation is faster (fewer dimensions = fewer distance\n",
    "> calculations).​  \n",
    "> ○​ Then KNN can work more effectively on the reduced feature space.​\n",
    ">\n",
    "> **Example of Complementary Use**\n",
    ">\n",
    "> ●​ Suppose we have an **image classification** problem with 1000+ pixel\n",
    "> features.​\n",
    ">\n",
    "> ●​ Running KNN directly would be slow and unreliable.​\n",
    ">\n",
    "> ●​ If we apply PCA and reduce features to, say, 50 principal\n",
    "> components:​\n",
    ">\n",
    "> ○​ The key structure of the images is preserved.​\n",
    ">\n",
    "> ○​ KNN becomes faster and more accurate since it works on meaningful,\n",
    "> compact features.\n",
    ">\n",
    "> **Question 6**: Train a KNN Classifier on the Wine dataset with and\n",
    "> without feature scaling. Compare model accuracy in both cases.\n",
    ">\n",
    "> **Answer 6 :-**  \n",
    "> Python Code (With and Without Scaling)  \n",
    "> import numpy as np  \n",
    "> from sklearn.datasets import load_wine  \n",
    "> from sklearn.model_selection import train_test_splitfrom\n",
    "> sklearn.preprocessing import StandardScalerfrom sklearn.neighbors\n",
    "> import KNeighborsClassifierfrom sklearn.metrics import accuracy_score\n",
    ">\n",
    "> \\# 1. Load dataset  \n",
    "> wine = load_wine()  \n",
    "> X, y = wine.data, wine.target\n",
    ">\n",
    "> \\# 2. Train-test split  \n",
    "> X_train, X_test, y_train, y_test = train_test_split(\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>X, y, test_size=0.2, random_state=42, stratify=y</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> )\n",
    ">\n",
    "> \\# -------------------------  \n",
    "> \\# Case 1: Without Scaling  \n",
    "> \\# -------------------------  \n",
    "> knn_no_scale =\n",
    "> KNeighborsClassifier(n_neighbors=5)knn_no_scale.fit(X_train,\n",
    "> y_train)  \n",
    "> y_pred_no_scale = knn_no_scale.predict(X_test)  \n",
    "> acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
    ">\n",
    "> \\# -------------------------  \n",
    "> \\# Case 2: With Scaling  \n",
    "> \\# -------------------------  \n",
    "> scaler = StandardScaler()  \n",
    "> X_train_scaled = scaler.fit_transform(X_train)  \n",
    "> X_test_scaled = scaler.transform(X_test)\n",
    ">\n",
    "> knn_scaled =\n",
    "> KNeighborsClassifier(n_neighbors=5)knn_scaled.fit(X_train_scaled,\n",
    "> y_train)  \n",
    "> y_pred_scaled = knn_scaled.predict(X_test_scaled)acc_scaled =\n",
    "> accuracy_score(y_test, y_pred_scaled)\n",
    ">\n",
    "> \\# Print results  \n",
    "> print(\"KNN Accuracy without Scaling:\", acc_no_scale)print(\"KNN\n",
    "> Accuracy with Scaling :\", acc_scaled)\n",
    ">\n",
    "> **Expected Results (Approximate)**  \n",
    "> ●​ **Without Scaling** → Accuracy around **0.70 – 0.75​**\n",
    ">\n",
    "> ●​ **With Scaling** → Accuracy improves to around **0.95 – 1.00\n",
    "> Question 7:** Train a PCA model on the Wine dataset and print the\n",
    "> explained variance ratio of each principal component.\n",
    ">\n",
    "> **Answer 7 :-**  \n",
    "> **Python Code**  \n",
    "> import numpy as np  \n",
    "> from sklearn.datasets import load_wine  \n",
    "> from sklearn.preprocessing import StandardScalerfrom\n",
    "> sklearn.decomposition import PCA\n",
    ">\n",
    "> \\# 1. Load dataset  \n",
    "> wine = load_wine()  \n",
    "> X, y = wine.data, wine.target\n",
    ">\n",
    "> \\# 2. Standardize features (important before PCA)scaler =\n",
    "> StandardScaler()  \n",
    "> X_scaled = scaler.fit_transform(X)\n",
    ">\n",
    "> \\# 3. Apply PCA  \n",
    "> pca = PCA()  \n",
    "> X_pca = pca.fit_transform(X_scaled)\n",
    ">\n",
    "> \\# 4. Print explained variance ratio  \n",
    "> print(\"Explained Variance Ratio of each Principal Component:\")for i,\n",
    "> ratio in enumerate(pca.explained_variance_ratio\\_):\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>print(f\"PC{i+1}: {ratio:.4f}\")</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> **Expected Output**  \n",
    "> Explained Variance Ratio of each Principal Component: PC1: 0.3619  \n",
    "> PC2: 0.1921  \n",
    "> PC3: 0.1111\n",
    ">\n",
    "> PC4: 0.0700  \n",
    "> PC5: 0.0656  \n",
    "> PC6: 0.0497  \n",
    "> PC7: 0.0411  \n",
    "> PC8: 0.0274  \n",
    "> PC9: 0.0223  \n",
    "> PC10: 0.0172  \n",
    "> PC11: 0.0155  \n",
    "> PC12: 0.0142  \n",
    "> PC13: 0.0120  \n",
    "> **Question 8:** Train a KNN Classifier on the PCA-transformed dataset\n",
    "> (retain top 2 components). Compare the accuracy with the original\n",
    "> dataset.\n",
    ">\n",
    "> **Answer 8 :-**\n",
    ">\n",
    "> **KNN on PCA-transformed Dataset vs Original**\n",
    ">\n",
    "> **Python Code**  \n",
    "> import numpy as np  \n",
    "> from sklearn.datasets import load_wine  \n",
    "> from sklearn.model_selection import train_test_splitfrom\n",
    "> sklearn.preprocessing import StandardScalerfrom sklearn.decomposition\n",
    "> import PCA  \n",
    "> from sklearn.neighbors import KNeighborsClassifier\n",
    ">\n",
    "> from sklearn.metrics import accuracy_score\n",
    ">\n",
    "> \\# 1. Load dataset  \n",
    "> wine = load_wine()  \n",
    "> X, y = wine.data, wine.target\n",
    ">\n",
    "> \\# 2. Train-test split  \n",
    "> X_train, X_test, y_train, y_test = train_test_split(\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>X, y, test_size=0.2, random_state=42, stratify=y</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> )\n",
    ">\n",
    "> \\# -------------------------  \n",
    "> \\# Case 1: KNN on original (scaled) data  \n",
    "> \\# -------------------------  \n",
    "> scaler = StandardScaler()  \n",
    "> X_train_scaled = scaler.fit_transform(X_train)  \n",
    "> X_test_scaled = scaler.transform(X_test)\n",
    ">\n",
    "> knn_original =\n",
    "> KNeighborsClassifier(n_neighbors=5)knn_original.fit(X_train_scaled,\n",
    "> y_train)  \n",
    "> y_pred_original = knn_original.predict(X_test_scaled)\n",
    ">\n",
    "> acc_original = accuracy_score(y_test, y_pred_original)\n",
    ">\n",
    "> \\# -------------------------  \n",
    "> \\# Case 2: KNN on PCA-transformed data (top 2 PCs)#\n",
    "> -------------------------  \n",
    "> pca = PCA(n_components=2)  \n",
    "> X_train_pca = pca.fit_transform(X_train_scaled)  \n",
    "> X_test_pca = pca.transform(X_test_scaled)\n",
    ">\n",
    "> knn_pca = KNeighborsClassifier(n_neighbors=5)  \n",
    "> knn_pca.fit(X_train_pca, y_train)  \n",
    "> y_pred_pca = knn_pca.predict(X_test_pca)  \n",
    "> acc_pca = accuracy_score(y_test, y_pred_pca)\n",
    ">\n",
    "> \\# Print results  \n",
    "> print(\"KNN Accuracy on Original Scaled Data:\", acc_original)print(\"KNN\n",
    "> Accuracy on PCA (2 components):\", acc_pca)\n",
    ">\n",
    "> **Expected Results**  \n",
    "> ●​ **KNN on original (scaled) data** → Accuracy ≈ **0.95 – 1.00​**\n",
    ">\n",
    "> ●​ **KNN on PCA (2 PCs)** → Accuracy ≈ **0.70 – 0.80**\n",
    ">\n",
    "> **Question 9:** Train a KNN Classifier with different distance metrics\n",
    "> (euclidean, manhattan) on the scaled Wine dataset and compare the\n",
    "> results.\n",
    ">\n",
    "> **Answer 9 :-**\n",
    ">\n",
    "> **KNN with Euclidean vs Manhattan Distance on Wine Dataset**\n",
    ">\n",
    "> **Python Code**  \n",
    "> import numpy as np  \n",
    "> from sklearn.datasets import load_wine  \n",
    "> from sklearn.model_selection import train_test_splitfrom\n",
    "> sklearn.preprocessing import StandardScalerfrom sklearn.neighbors\n",
    "> import KNeighborsClassifierfrom sklearn.metrics import accuracy_score\n",
    ">\n",
    "> \\# 1. Load dataset  \n",
    "> wine = load_wine()  \n",
    "> X, y = wine.data, wine.target\n",
    ">\n",
    "> \\# 2. Train-test split  \n",
    "> X_train, X_test, y_train, y_test = train_test_split(\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>X, y, test_size=0.2, random_state=42, stratify=y</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> )\n",
    ">\n",
    "> \\# 3. Scale features  \n",
    "> scaler = StandardScaler()  \n",
    "> X_train_scaled = scaler.fit_transform(X_train)  \n",
    "> X_test_scaled = scaler.transform(X_test)\n",
    ">\n",
    "> \\# -------------------------  \n",
    "> \\# Case 1: Euclidean Distance (p=2)  \n",
    "> \\# -------------------------  \n",
    "> knn_euclidean = KNeighborsClassifier(n_neighbors=5,\n",
    "> metric='minkowski',p=2)  \n",
    "> knn_euclidean.fit(X_train_scaled, y_train)  \n",
    "> y_pred_euclidean = knn_euclidean.predict(X_test_scaled)  \n",
    "> acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    ">\n",
    "> \\# -------------------------  \n",
    "> \\# Case 2: Manhattan Distance (p=1)  \n",
    "> \\# -------------------------  \n",
    "> knn_manhattan = KNeighborsClassifier(n_neighbors=5,metric='minkowski',\n",
    "> p=1)  \n",
    "> knn_manhattan.fit(X_train_scaled, y_train)  \n",
    "> y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
    ">\n",
    "> acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    ">\n",
    "> \\# Print results  \n",
    "> print(\"KNN Accuracy (Euclidean Distance):\", acc_euclidean)print(\"KNN\n",
    "> Accuracy (Manhattan Distance):\", acc_manhattan)\n",
    ">\n",
    "> **Expected Results (Approximate)**  \n",
    "> ●​ **Euclidean (p=2)** → Accuracy ≈ **0.95 – 1.00​**\n",
    ">\n",
    "> ●​ **Manhattan (p=1)** → Accuracy ≈ **0.90 – 0.95**  \n",
    "> **Question 10**: You are working with a high-dimensional gene\n",
    "> expression dataset to classify patients with different types of\n",
    "> cancer. Due to the large number of features and a small number of\n",
    "> samples, traditional models overfit. Explain how you would:  \n",
    "> ● Use PCA to reduce dimensionality  \n",
    "> ● Decide how many components to keep  \n",
    "> ● Use KNN for classification post-dimensionality reduction  \n",
    "> ● Evaluate the model  \n",
    "> ● Justify this pipeline to your stakeholders as a robust solution\n",
    "> for  \n",
    "> real-world biomedical data  \n",
    "> **Answer 10 :-**\n",
    ">\n",
    "> **Problem Context**\n",
    ">\n",
    "> **●​ Gene expression datasets typically have thousands of features**\n",
    "> **(genes) but only a few hundred patients (samples).​**\n",
    ">\n",
    "> **●​ This imbalance causes overfitting in traditional models, since**\n",
    "> **they try to fit noise in high-dimensional space.​**\n",
    ">\n",
    "> **●​ To handle this, we combine PCA for dimensionality reduction**\n",
    "> **with KNN for classification.​**\n",
    ">\n",
    "> **Step 1: Use PCA to Reduce Dimensionality**\n",
    ">\n",
    "> ●​ PCA projects the high-dimensional gene expression data into a\n",
    "> lower-dimensional space.​\n",
    ">\n",
    "> ●​ It keeps the directions of **maximum variance (principal**\n",
    "> **components)** while discarding noise and redundancy.\n",
    ">\n",
    "> **Step 2: Decide How Many Components to Keep**\n",
    ">\n",
    "> ●​ We use the **explained variance ratio** to decide.​\n",
    ">\n",
    "> ●​ A **scree plot (cumulative variance)** helps select the smallest\n",
    "> number of components that explain **90–95% variance**.\n",
    ">\n",
    "> **Step 3: KNN for Classification**\n",
    ">\n",
    "> ●​ After reducing dimensionality, train a **KNN classifier** on the\n",
    "> transformed data.​\n",
    ">\n",
    "> ●​ KNN benefits from PCA because distances are more meaningful in low\n",
    "> dimensions.\n",
    ">\n",
    "> **Step 4: Model Evaluation**\n",
    ">\n",
    "> ●​ Use **cross-validation** on the training set to tune parameters\n",
    "> (neighbors, distance metric).​\n",
    ">\n",
    "> ●​ Evaluate on a held-out **test set** using **accuracy, confusion\n",
    "> matrix,** **and classification report**.\n",
    ">\n",
    "> **Step 5: Justification to Stakeholders**  \n",
    "> ●​ **Robustness**: PCA reduces noise, avoids overfitting, and speeds up\n",
    "> computation.​\n",
    ">\n",
    "> ●​ **Interpretability**: PCA compresses thousands of genes into a few\n",
    "> components while retaining most variance.​\n",
    ">\n",
    "> ●​ **Accuracy**: KNN works better in reduced space, providing reliable\n",
    "> cancer classification.​\n",
    ">\n",
    "> ●​ **Generalizability**: The pipeline is less likely to overfit and\n",
    "> more likely to work on new patients.\n",
    ">\n",
    "> **<u>Python Implementation</u>**  \n",
    "> import numpy as np  \n",
    "> import pandas as pd  \n",
    "> from sklearn.datasets import make_classification  \n",
    "> from sklearn.model_selection import train_test_split,\n",
    "> StratifiedKFold,GridSearchCV  \n",
    "> from sklearn.preprocessing import StandardScaler  \n",
    "> from sklearn.decomposition import PCA  \n",
    "> from sklearn.pipeline import Pipeline  \n",
    "> from sklearn.neighbors import KNeighborsClassifier\n",
    ">\n",
    "> from sklearn.metrics import accuracy_score,\n",
    "> classification_report,confusion_matrix  \n",
    "> import matplotlib.pyplot as plt\n",
    ">\n",
    "> \\# 1. Simulate a high-dimensional gene expression datasetX, y =\n",
    "> make_classification(\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>n_samples=180, # few patients</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>n_features=5000, # thousands of genes</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>n_informative=60,</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>n_classes=3,</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>class_sep=2.0,</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>random_state=42</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> )\n",
    ">\n",
    "> X_train, X_test, y_train, y_test = train_test_split(\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>X, y, test_size=0.3, stratify=y, random_state=42</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> )\n",
    ">\n",
    "> \\# 2. PCA for variance analysis  \n",
    "> scaler = StandardScaler()  \n",
    "> X_train_scaled = scaler.fit_transform(X_train)\n",
    ">\n",
    "> pca_full = PCA().fit(X_train_scaled)  \n",
    "> cum_var = np.cumsum(pca_full.explained_variance_ratio\\_)\n",
    ">\n",
    "> plt.figure(figsize=(7,4))  \n",
    "> plt.plot(np.arange(1, len(cum_var)+1), cum_var,\n",
    "> marker=\".\")plt.axhline(y=0.95, color=\"r\", linestyle=\"--\")  \n",
    "> plt.xlabel(\"Number of Principal Components\")  \n",
    "> plt.ylabel(\"Cumulative Explained Variance\")  \n",
    "> plt.title(\"Scree Plot - PCA on Gene Expression Data\")  \n",
    "> plt.show()\n",
    ">\n",
    "> \\# 3. Build PCA + KNN pipeline  \n",
    "> pipe = Pipeline(\\[\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>(\"scaler\", StandardScaler()),</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>(\"pca\", PCA(svd_solver=\"full\", random_state=42)),</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>(\"knn\", KNeighborsClassifier())</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> \\])\n",
    ">\n",
    "> \\# Hyperparameter tuning  \n",
    "> param_grid = {\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"pca__n_components\": [0.90, 0.95, 0.99], # keep 90%, 95%, 99%</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> variance\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"knn__n_neighbors\": [3, 5, 7, 9],</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"knn__p\": [1, 2], # Manhattan vs Euclidean</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>\"knn__weights\": [\"uniform\", \"distance\"]</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> }\n",
    ">\n",
    "> cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)grid =\n",
    "> GridSearchCV(pipe, param_grid, scoring=\"accuracy\", cv=cv,n_jobs=-1)  \n",
    "> grid.fit(X_train, y_train)\n",
    ">\n",
    "> \\# 4. Evaluate on test set  \n",
    "> best_model = grid.best_estimator\\_  \n",
    "> y_pred = best_model.predict(X_test)\n",
    ">\n",
    "> print(\"Best Parameters:\", grid.best_params\\_)  \n",
    "> print(\"Best CV Accuracy: {:.3f}\".format(grid.best_score\\_))  \n",
    "> print(\"Test Accuracy: {:.3f}\".format(accuracy_score(y_test,\n",
    "> y_pred)))print(\"\\nClassification Report:\\n\",\n",
    "> classification_report(y_test, y_pred))print(\"Confusion Matrix:\\n\",\n",
    "> confusion_matrix(y_test, y_pred))\n",
    ">\n",
    "> **Expected Results**\n",
    ">\n",
    "> ●​ **Scree Plot** → \\~100–150 components needed to capture 95%\n",
    "> variance.​\n",
    ">\n",
    "> ●​ **Best Params** → something like:\n",
    ">\n",
    "> {'pca\\_\\_n_components': 0.95, 'knn\\_\\_n_neighbors': 5, 'knn\\_\\_p': 2,\n",
    "> 'knn\\_\\_weights': 'distance'}\n",
    ">\n",
    "> ●​ **Accuracy** →  \n",
    "> ●​ CV Accuracy ≈ **0.85 – 0.90​**\n",
    ">\n",
    "> ●​ Test Accuracy ≈ **0.80 – 0.90**"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
